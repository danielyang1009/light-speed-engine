# Volume 56, Issue 11
- November 2010
- Pages iv-2110
- David Simchi-Levi

## 1. Management Insights
### Author(s):
- Michael F. Gorman
### Published:
- 1 Nov 2010
### Abstract:
Xiaofang Wang, Laurens G. Debo, Alan Scheller-Wolf, Stephen F. Smith
### Link:
- https://doi.org/10.1287/mnsc.1100.1276

## 2. Design and Analysis of Diagnostic Service Centers
### Author(s):
- Xiaofang Wang
- Laurens G. Debo
- Alan Scheller-Wolf
- Stephen F. Smith
### Published:
- 1 Nov 2010
### Abstract:
In the health-care domain, diagnostic service centers provide advice to patients over the phone about what the most appropriate course of action is based on their symptoms. Managers of such centers must strike a balance between accuracy of advice, callers' waiting time, and staffing costs by setting the appropriate capacity (staffing) and service depth. We model this problem as a multiple-server queueing system, with the servers performing a sequential testing process and the customers deciding whether or not to use the service, based on their expectation of accuracy and congestion. We find the dual concerns of accuracy and congestion lead to a counterintuitive impact of capacity: Increasing capacity might increase congestion. In addition, (i) patient population size is an important driver in management decisions, not only in staffing but also in accuracy of advice; (ii) increasing asymmetry in error costs may not increase asymmetry in the corresponding error rates; and (iii) the error costs for the two major stakeholders—the service manager and the patient—may impact the optimal staffing level in different ways. Finally, we highlight the relevance of our model and results to challenges in practice elicited during interviews with current clinical researchers and practitioners.
### Link:
- https://doi.org/10.1287/mnsc.1100.1236

## 3. Reference Dependence in Multilocation Newsvendor Models: A Structural Analysis
### Author(s):
- Teck-Hua Ho
- Noah Lim
- Tony Haitao Cui
### Published:
- 11 Oct 2010
### Abstract:
We propose a behavioral theory to predict actual ordering behavior in multilocation inventory systems. The theory rests on a well-known stylized fact of human behavior: people's preferences are reference dependent. We incorporate reference dependence into the newsvendor framework by assuming that there are psychological costs of leftovers and stockouts. We also hypothesize that the psychological aversion to leftovers is greater than the disutility for stockouts. We then experimentally test the proposed theory in both the centralized and decentralized inventory structures using subjects motivated by substantial financial incentives. Consistent with the proposed theory, actual orders exhibit the so-called “pull-to-center” bias and the degree of bias is greater in the high-profit margin than in the low-profit margin condition. These systematic biases are shown to eliminate the risk-pooling benefit when the demands across store locations are strongly correlated. Because the proposed model nests the standard inventory and ex post inventory error minimization theories as special cases, one can systematically evaluate the predictive power of each alternative using the generalized likelihood principle. We structurally estimate all three theories using the experimental data, and the estimation results strongly suggest that the proposed behavioral theory captures actual orders and profits better. We also conduct two experiments to validate the behavioral model by manipulating the relative salience of the psychological costs of leftovers versus that of stockouts to alleviate the pull-to-center bias.
### Link:
- https://doi.org/10.1287/mnsc.1100.1225

## 4. Manipulation Robustness of Collaborative Filtering
### Author(s):
- Benjamin Van Roy
- Xiang Yan
### Published:
- 1 Nov 2010
### Abstract:
A collaborative filtering system recommends to users products that similar users like. Collaborative filtering systems influence purchase decisions and hence have become targets of manipulation by unscrupulous vendors. We demonstrate that nearest neighbors algorithms, which are widely used in commercial systems, are highly susceptible to manipulation and introduce new collaborative filtering algorithms that are relatively robust.
### Link:
- https://doi.org/10.1287/mnsc.1100.1232

## 5. Technology Usage and Online Sales: An Empirical Study
### Author(s):
- Prabuddha De
- Yu (Jeffrey) Hu
- Mohammad S. Rahman
### Published:
- 11 Oct 2010
### Abstract:
Despite the widespread adoption of search and recommendation technologies on the Internet, empirical research that examines the effect of these technologies is scarce. How do online consumers use these technologies? Does consumers' technology usage have an effect on the sales to them or their purchasing patterns? This paper empirically measures consumers' usage of website technologies by analyzing server log data. We match technology usage data to sales data, controlling for consumers' historical purchasing behavior. Our unique data set allows us to reveal the relationship between technology usage and online sales. Our analyses show that consumers' information technology usage has a significant effect on the sales to them, but this effect varies for different technologies and across different products. In particular, the use of directed search has a positive effect on the sales of promoted products, whereas it has a negative effect on the sales of nonpromoted products. In contrast, the use of a recommendation system has a positive effect on the sales of both promoted and nonpromoted products. Surprisingly, the use of nondirected search has an insignificant effect on online sales.
### Link:
- https://doi.org/10.1287/mnsc.1100.1233

## 6. Trading Higher Software Piracy for Higher Profits: The Case of Phantom Piracy
### Author(s):
- Ram D. Gopal
- Alok Gupta
### Published:
- 9 Sep 2010
### Abstract:
Faced with the sustained problem of piracy that costs nearly $40 billion in annual revenue losses, the software industry has adopted a number of technical, legal, and economic strategies to curb piracy and stem the resulting losses. Our work complements and contributes to the existing literature by exploring the possible effect of another economic lever—product bundling—on the relationship governing piracy and seller profits. The traditional economic rationale of demand pooling from bundling that enables sellers to extract higher surplus and its particular attractiveness for information goods with negligible marginal and bundling costs carry over to our analysis. However, the presence of piracy injects several new facets to our analysis. Bundling creates a shared level of piracy of disparate products, and under certain conditions to the detriment of one of the products. We argue that by construction of the copyright laws, the act of bundling itself can have a deterrence effect. This deterrence effect, along with shared piracy of products and demand pooling are ingredients that together dictate the overall piracy, pricing, profit, and welfare outcomes. Our analysis reveals several interesting insights. Bundling can be profitable even when the very act of bundling increases the piracy level of one of the products in the bundle. Termed phantom piracy, this represents a situation where sellers trade off higher piracy for one product in favor of lower piracy for the other product while deriving overall higher profits. Extensive simulation analysis shows that the region of phantom piracy is vastly expanded when additional products are introduced to the bundle. Conversely, under certain conditions, a profit maximizing seller opts not to bundle even when bundling can serve to lower the overall level of piracy. Price discounts that are typically offered by bundling are sharply deepened when piracy enters the equation. When piracy is a phenomenon to contend with, product bundling always increases consumer surplus even in scenarios where the seller may not realize higher profits. Unlike other forms of price discrimination that are often viewed by consumers with a jaundiced eye as they attempt to extract additional surplus from the consumers, product bundling in the software context can be a win-win scenario for both the buyers and the sellers.
### Link:
- https://doi.org/10.1287/mnsc.1100.1221

## 7. Optimal Bundling Strategies Under Heavy-Tailed Valuations
### Author(s):
- Rustam Ibragimov
- Johan Walden
### Published:
- 11 Oct 2010
### Abstract:
We develop a framework for the optimal bundling problem of a multiproduct monopolist, who provides goods to consumers with private valuations that are random draws from a distribution with heavy tails. We show that in the Vickrey auction setting, the buyers prefer separate provision of the goods to any bundles. We also provide a complete characterization of the optimal bundling strategies for a monopolist producer, who provides goods for profit-maximizing prices. For products with low marginal costs, the seller's optimal strategy is to provide goods separately when consumers' valuations are heavy-tailed and in a single bundle when valuations are thin-tailed. These conclusions are reversed for goods with high marginal costs. For simplicity, we use a specific class of independent and identically distributed random variables, but our results can be generalized to include dependence, skewness, and the case of nonidentical one-dimensional distributions.
### Link:
- https://doi.org/10.1287/mnsc.1100.1234

## 8. Prediction Markets: Alternative Mechanisms for Complex Environments with Few Traders
### Author(s):
- Paul J. Healy
- Sera Linardi
- J. Richard Lowery
- John O. Ledyard
### Published:
- 11 Oct 2010
### Abstract:
Double auction prediction markets have proven successful in large-scale applications such as elections and sporting events. Consequently, several large corporations have adopted these markets for smaller-scale internal applications where information may be complex and the number of traders is small. Using laboratory experiments, we test the performance of the double auction in complex environments with few traders and compare it to three alternative mechanisms. When information is complex we find that an iterated poll (or Delphi method) outperforms the double auction mechanism. We present five behavioral observations that may explain why the poll performs better in these settings.
### Link:
- https://doi.org/10.1287/mnsc.1100.1226

## 9. Optimal Portfolio Liquidation with Distress Risk
### Author(s):
- David B. Brown
- Bruce Ian Carlin
- Miguel Sousa Lobo
### Published:
- 1 Nov 2010
### Abstract:
We analyze the problem of an investor who needs to unwind a portfolio in the face of recurring and uncertain liquidity needs, with a model that accounts for both permanent and temporary price impact of trading. We first show that a risk-neutral investor who myopically deleverages his position to meet an immediate need for cash always prefers to sell more liquid assets. If the investor faces the possibility of a downstream shock, however, the solution differs in several important ways. If the ensuing shock is sufficiently large, the nonmyopic investor unwinds positions more than immediately necessary and, all else being equal, prefers to retain more of the assets with low temporary price impact in order to hedge against possible distress. More generally, optimal liquidation involves selling strictly more of the assets with a lower ratio of permanent to temporary impact, even if these assets are relatively illiquid. The results suggest that properly accounting for the possibility of future shocks should play a role in managing large portfolios.
### Link:
- https://doi.org/10.1287/mnsc.1100.1235

## 10. Time-Tradeoff Sequences for Analyzing Discounting and Time Inconsistency
### Author(s):
- Arthur E. Attema
- Han Bleichrodt
- Kirsten I. M. Rohde
- Peter P. Wakker
### Published:
- 11 Oct 2010
### Abstract:
This paper introduces time-tradeoff (TTO) sequences as a general tool to analyze intertemporal choice. We give several applications. For empirical purposes, we can measure discount functions without requiring any measurement of or assumption about utility. We can quantitatively measure time inconsistencies and simplify their qualitative tests. TTO sequences can be administered and analyzed very easily, using only pencil and paper. For theoretical purposes, we use TTO sequences to axiomatize (quasi-)hyperbolic discount functions. We demonstrate the feasibility of measuring TTO sequences in an experiment, in which we tested the axiomatizations. Our findings suggest rejections of several currently popular discount functions and call for the development of new ones. It is especially desirable that such discount functions can accommodate increasing impatience.
### Link:
- https://doi.org/10.1287/mnsc.1100.1219

## 11. Conditional Coskewness in Stock and Bond Markets: Time-Series Evidence
### Author(s):
- Jian Yang
- Yinggang Zhou
- Zijun Wang
### Published:
- 11 Oct 2010
### Abstract:
In the context of a three-moment intertemporal capital asset pricing model specification, we characterize conditional coskewness between stock and bond excess returns using a bivariate regime-switching model. We find that both conditional U.S. stock coskewness (the relation between stock return and bond volatility) and bond coskewness (the relation between bond return and stock volatility) command statistically and economically significant negative ex ante risk premiums. The impacts of stock and bond coskewness on the conditional stock and bond premiums are quite robust to various model specifications and various sample periods, and also hold in another major developed country (the United Kingdom). The findings also carry important implications for portfolio management.
### Link:
- https://doi.org/10.1287/mnsc.1100.1237

## 12. Modifying the Mean-Variance Approach to Avoid Violations of Stochastic Dominance
### Author(s):
- Pavlo R. Blavatskyy
### Published:
- 9 Sep 2010
### Abstract:
The mean-variance approach is an influential theory of decision under risk proposed by Markowitz (Markowitz, H. 1952. Portfolio selection. J. Finance7(1) 77–91). The mean-variance approach implies violations of first-order stochastic dominance not commonly observed in the data. This paper proposes a new model in the spirit of the classical mean-variance approach without violations of stochastic dominance. The proposed model represents preferences by a functional U(L) − ρ · r(L), where U(L) denotes the expected utility of lottery L, ρ ∈ [−1, 1] is a subjective constant, and r(L) is the mean absolute (utility) semideviation of lottery L. The model comprises a linear trade-off between expected utility and utility dispersion. The model can accommodate several behavioral regularities such as the Allais paradox and switching behavior in Samuelson's example.
### Link:
- https://doi.org/10.1287/mnsc.1100.1224

## 13. On the Number of State Variables in Options Pricing
### Author(s):
- Gang Li
- Chu Zhang
### Published:
- 3 Sep 2010
### Abstract:
In this paper, we investigate the methodological issue of determining the number of state variables required for options pricing. After showing the inadequacy of the principal component analysis approach, which is commonly used in the literature, we adopt a nonparametric regression technique with nonlinear principal components extracted from the implied volatilities of various moneyness and maturities as proxies for the transformed state variables. The methodology is applied to the prices of S&P 500 index options from the period 1996–2005. We find that, in addition to the index value itself, two state variables, approximated by the first two nonlinear principal components, are adequate for pricing the index options and fitting the data in both time series and cross sections.
### Link:
- https://doi.org/10.1287/mnsc.1100.1222

## 14. Target Age and the Acquisition of Innovation in High-Technology Industries
### Author(s):
- Sam Ransbotham
- Sabyasachi Mitra
### Published:
- 3 Sep 2010
### Abstract:
External acquisition of new technology is a growing trend in the innovation and product development process, particularly in high-technology industries, as firms complement internal research and development efforts with aggressive acquisition programs. Yet, despite its importance, there has been little empirical research on the timing of acquisition decisions in high-technology environments. Should organizations wait until more information is available about the target and its markets so that a better valuation can be obtained? Or should the target be acquired early to lower acquisition cost and gain early access to key technologies? Applying an event study methodology to technology acquisitions in the telecommunications industry from 1995 to 2001, we find evidence that supports acquiring early in the face of uncertainty. Our analytical model and empirical analysis uncover two characteristics of young targets that drive benefits from early acquisitions—flexible growth options that provide greater opportunities for synergistic fit, and greater valuation uncertainty that leads to lower prices. However, the negative effect of target age on acquirer value is partially mitigated if the target has recent patents or is privately held. In addition, the probability of acquisition is higher for targets that have signals of higher quality, and lower for targets that have superior access to capital and resources.
### Link:
- https://doi.org/10.1287/mnsc.1100.1223

## 15. Network-Independent Partner Selection and the Evolution of Innovation Networks
### Author(s):
- Joel A. C. Baum
- Robin Cowan
- Nicolas Jonard
### Published:
- 11 Oct 2010
### Abstract:
Empirical research on strategic alliances has focused on the idea that partners are selected on the basis of social capital considerations. In this paper we emphasize instead the role of complementary knowledge stocks and knowledge dynamics, which have received surprisingly limited attention relative to social capital as forces behind the formation and dynamics of innovation networks. To marshal evidence in this regard, we design a simple model of partner selection in which firms ally for the purpose of learning and innovating, and in doing so create an industry network. We abstract completely from network-based structural and strategic motives for partner selection and focus instead on the idea that firms' knowledge bases must “fit” for joint learning and innovation to be possible, and thus for an alliance to be feasible. The striking result is that, despite containing no social capital considerations, this simple model replicates the firm conduct, network structure, and contingent effects of network position on performance observed and discussed in the empirical literature.
### Link:
- https://doi.org/10.1287/mnsc.1100.1229


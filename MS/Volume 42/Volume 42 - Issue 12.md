# Volume 42, Issue 12
- December 1996
- Pages 1627-1752
- David Simchi-Levi

## 1. Network Externalities in Microcomputer Software: An Econometric Analysis of the Spreadsheet Market
### Author(s):
- Erik Brynjolfsson
- Chris F. Kemerer
### Published:
- 1 Dec 1996
### Abstract:
Because of network externalities, the success of a software product may depend in part on stalled base and its conformance to industry standards. This research builds a hedonic model to determine the effects of network externalities, standards, intrinsic features and a time trend on microcomputer spreadsheet software prices. When data for a sample of products during the 19871992 time period were analyzed using this model, four main results emerged: 1) Network externalities, as measured by the size of a product's installed base, significantly increased the price of spreadsheet products: a one percent increase in a product's installed base was associated with a 0.75% increase in its price. 2) Products which adhered to the dominant standard, the Lotus menu tree interface, commanded prices which were higher by an average of 46%. 3) Although nominal prices increased slightly during this time period, quality-adjusted prices declined by an average of 16% per year. 4) The hedonic model was found to be a good predictor of actual market prices, despite the fact that it was originally estimated using list prices. Several variations of the model were examined, and, while the qualitative findings were robust, the precise estimates of the coefficients varied somewhat depending on the sample of products examined, the weighting of the observations and the functional form used in estimation, suggesting that the use of hedonic methods in this domain is subject to a number of limitations due, inter alia, to the potential for strategic pricing by vendors.
### Link:
- https://doi.org/10.1287/mnsc.42.12.1627

## 2. Organizational Consultant: Creating a Useable Theory for Organizational Design
### Author(s):
- Helmy H. Baligh
- Richard M. Burton
- Brge Obel
### Published:
- 1 Dec 1996
### Abstract:
Organization theory is a positive science; organizational design is a normative science concerned with how things ought to be, with devising structures to attain goals. The Organizational Consultant is a knowledge base expert system to help design organizations. That is, it takes facts about the environment, size, strategy, technology, ownership, and management preferences and applies the knowledge base to recommend the design structure and properties such as complexity, formalization, centralization, and span of control, among others.
### Link:
- https://doi.org/10.1287/mnsc.42.12.1648

## 3. Verification of Uncertain Knowledge-Based Systems: An Empirical Verification Approach
### Author(s):
- Daniel E. O'Leary
### Published:
- 1 Dec 1996
### Abstract:
A number of different tests and approaches are developed to determine the existence of potential anomalies in rule-based systems that employ MYCIN uncertainty factors (weights). First, the distribution of weights is compared to other systems' distributions and weights are investigated as to their individual meanings, to determine whether any weights are unusual. Second, there is increasing evidence that people are not good at developing weights on rules, building in symmetries and redundancies that signal usual assumptions about the underlying probabilities. Accordingly, weight symmetries generated from rule pairs are analyzed to determine the existence of anomalies. Third, typically rule-based tools have been developed for application in specific domains, such as medicine. Unique aspects of those domains may limit application of the tools to other domains. Finally, ad hoc, rule-based approaches are suboptimal, and alternative formal probability approaches, such as Bayes' nets, more fully specify the probabilistic nature of knowledge.
### Link:
- https://doi.org/10.1287/mnsc.42.12.1663

## 4. Curvature of the Probability Weighting Function
### Author(s):
- George Wu
- Richard Gonzalez
### Published:
- 1 Dec 1996
### Abstract:
When individuals choose among risky alternatives, the psychological weight attached to an outcome may not correspond to the probability of that outcome. In rank-dependent utility theories, including prospect theory, the probability weighting function permits probabilities to be weighted nonlinearly. Previous empirical studies of the weighting function have suggested an inverse S-shaped function, first concave and then convex. However, these studies suffer from a methodological shortcoming: estimation procedures have required assumptions about the functional form of the value and/or weighting functions. We propose two preference conditions that are necessary and sufficient for concavity and convexity of the weighting function. Empirical tests of these conditions are independent of the form of the value function. We test these conditions using preference ladders (a series of questions that differ only by a common consequence). The concavity-convexity ladders validate previous findings of an S-shaped weighting function, concave up to p < 0.40, and convex beyond that probability. The tests also show significant nonlinearity away from the boundaries, 0 and 1. Finally, we fit the ladder data with weighting functions proposed by Tversky and Kahneman (Tversky, Amos, Daniel Kahneman. 1992. Advances in prospect theory: Cumulative representation of uncertainty. J. Risk and Uncertainty5 297323.) and Prelec (Prelec, Draen. 1995. The probability weighting function. Unpublished paper.).
### Link:
- https://doi.org/10.1287/mnsc.42.12.1676

## 5. A Standard Measure of Risk and Risk-Value Models
### Author(s):
- Jianmin Jia
- James S. Dyer
### Published:
- 1 Dec 1996
### Abstract:
In this paper we propose a standard measure of risk that is based on the converted expected utility of normalized lotteries with zero-expected values. This measure of risk has many desirable properties that characterize the notion of risk. It is very general and includes many previously proposed measures of risk as special cases. Moreover, our standard measure of risk provides a preference-based and unified method for risk studies. Since the standard measure of risk is compatible with the measure of expected utility, it can be used explicitly or implicitly in an expected utility model. Under a condition called risk Independence, a decision could be made by explicitly trading off between risk and value, which offers an alternative representation of the expected utility model, named the standard risk-value model. Finally, we discuss some other applications of the standard measure of risk and extensions of our risk-value tradeoff framework for descriptive decision making.
### Link:
- https://doi.org/10.1287/mnsc.42.12.1691

## 6. The Dynamic and Stochastic Knapsack Problem with Deadlines
### Author(s):
- Jason D. Papastavrou
- Srikanth Rajagopalan
- Anton J. Kleywegt
### Published:
- 1 Dec 1996
### Abstract:
In this paper a dynamic and stochastic model of the well-known knapsack problem is developed and analyzed. The problem is motivated by a wide variety of real-world applications. Objects of random weight and reward arrive according to a stochastic process in time. The weights and rewards associated with the objects are distributed according to a known probability distribution. Each object can either be accepted to be loaded into the knapsack, of known weight capacity, or be rejected. The objective is to determine the optimal policy for loading the knapsack within a fixed time horizon so as to maximize the expected accumulated reward. The optimal decision rules are derived and are shown to exhibit surprising behavior in some cases. It is also shown that if the distribution of the weights is concave, then the decision rules behave according to intuition.
### Link:
- https://doi.org/10.1287/mnsc.42.12.1706

## 7. Combining Interior-Point and Pivoting Algorithms for Linear Programming
### Author(s):
- Erling D. Andersen
- Yinyu Ye
### Published:
- 1 Dec 1996
### Abstract:
We propose a new approach to combine linear programming (LP) interior-point and simplex pivoting algorithms. In any iteration of an interior-point algorithm we construct a related LP problem, which approximates the original problem, with a known (strictly) complementary primal-dual solution pair. Thus, we can apply Megiddo's (Megiddo, N. 1991. On finding primal- and dual-optimal bases. ORSA J. Comput.3(1) 6365.) pivoting procedure to compute an optimal basis for the approximate problem in strongly polynomial time. We show that, if the approximate problem is constructed from an interior-point iterate sufficiently close to the optimal face, then any optimal basis of the approximate problem is an optimal basis for the original problem. If the LP data are rational, the total number of interior-point iterations to create such a sufficient approximate problem is bounded by a polynomial in the data size. We develop a modification of Megiddo's procedure and discuss several implementation issues in solving the approximate problem. We also report encouraging computational results for this combined approach.
### Link:
- https://doi.org/10.1287/mnsc.42.12.1719

## 8. Learning in Setups: Analysis, Minimal Forecast Horizons, and Algorithms
### Author(s):
- Michal Tzur
### Published:
- 1 Dec 1996
### Abstract:
We analyze the dynamic lot-sizing model in which the cost of a setup depends on the number of setups that have occurred prior to it. This arises, for example, when there exist learning effects in setups. Our model is more general than most learning models in the literature since it allows the total setup cost to be a general nondecreasing (but not necessarily concave) function of the number of setups.
### Link:
- https://doi.org/10.1287/mnsc.42.12.1732

## 9. Interval Coverage in Multiclass Queues Using Batch Mean Estimates
### Author(s):
- M. Eric Johnson
- John Jackman
### Published:
- 1 Dec 1996
### Abstract:
We investigate the fixed, sample-size, batch-mean procedure for creating confidence intervals from simulated data obtained from a stochastic queueing system with multiple customer classes. We show that, for a multiclass M/M/1 queue, serial correlation between customers of the same class decreases to zero as the number of customer classes increases. We also derive a closed-form expression for the asymptotic variance of waiting time by customer type. We then empirically examine batch-mean estimator coverage for a simple queue with multiple customer classes. We find that batch-mean estimators perform better in terms of coverage and interval half width in multiclass queues, with a fixed number of observations per class, than in the traditionally studied single-class systems. We also examine the effect of multiple classes where the total computational effort remains fixed.
### Link:
- https://doi.org/10.1287/mnsc.42.12.1744


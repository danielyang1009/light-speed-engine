# Volume 45, Issue 02
- February 1999
- Pages 131-295
- David Simchi-Levi

## 1. Pricing Patterns of Cellular Phones and Phonecalls: A Segment-Level Analysis
### Author(s):
- Dipak C. Jain
- Eitan Muller
- Naufel J. Vilcassim
### Published:
- 1 Feb 1999
### Abstract:
One expectation of the U.S. Federal Communications Commission (FCC) in the early stages of the cellular communications industry was that the presence of two licensees in each market would ensure competition, and thereby result in declining prices over time for both cellular phones (handsets) and phonecalls. However, industry observers have noted recently that although the price of handsets has declined over time, the price of the phonecalls has not. We investigate this interesting pricing issue by modeling the market interaction between the providers of cellular services and also their interaction with customers using a game theoretic framework.
### Link:
- https://doi.org/10.1287/mnsc.45.2.131

## 2. New Firm Survival: Institutional Explanations for New Franchisor Mortality
### Author(s):
- Scott Shane
- Maw-Der Foo
### Published:
- 1 Feb 1999
### Abstract:
Why do some new firms succeed and others fail? Economists argue that new firms fail because entrepreneurs inefficiently manage production and organizational design (Williamson 1985). Sociologists (e.g., Granovetter 1985) have typically viewed this explanation as undersocialized, and argue that institutional legitimacy must also be considered to explain the survival of new firms. This paper examines the survival of 1292 new franchisors established in the United States from 19791996. The results show that institutional legitimacy adds to economic explanations for the survival of new franchisors and suggests the importance of a properly socialized explanation.
### Link:
- https://doi.org/10.1287/mnsc.45.2.142

## 3. A Punctuated-Equilibrium Model of Technology Diffusion
### Author(s):
- Christoph H. Loch
- Bernardo A. Huberman
### Published:
- 1 Feb 1999
### Abstract:
We present an evolutionary model of technology diffusion in which an old and a new technology are available, both of which improve their performance incrementally over time. Technology adopters make repeated choices between the established and the new technology based on their perceived performance, which is subject to uncertainty. Both technologies exhibit positive externalities, or performance benefits from others using the same technology.
### Link:
- https://doi.org/10.1287/mnsc.45.2.160

## 4. Centralization of Stocks: Retailers vs. Manufacturer
### Author(s):
- Ravi Anupindi
- Yehuda Bassok
### Published:
- 1 Feb 1999
### Abstract:
A well-known result in inventory theory is that physical centralization of stocks in a system with multiple retailers decreases total costs and increases total profits for the retailers. However, does this centralization also benefit the manufacturer, whose goods the retailers stock, when customers unsatisfied at retailers due to stock-outs are considered lost sales? In this paper we consider a model with two retailers and one manufacturer. We then compare two systems: one in which the retailers hold stocks separately and the other in which they cooperate to centralize stocks at a single location. We show that whether or not centralization of stocks by retailers increases profits for the manufacturer depends on the level of market search in the supply chain. Market search is measured as the fraction of customers who, unsatisfied at their local retailer due to a stock-out, search for the good at the other retailer before leaving the system. Specifically, we show that there exists a threshold level for market search above which the manufacturer loses. Furthermore, for very high search levels, even the system profit (sum of manufacturer and retailer profits) may decrease upon centralization. We then compare the performance of the two systems under optimal pricing/subsidy mechanisms and show that often a manufacturer is better off in a decentralized system with high market search. We conclude with a discussion of the role of information systems in the decentralized systems.
### Link:
- https://doi.org/10.1287/mnsc.45.2.178

## 5. Improving Service by Informing Customers About Anticipated Delays
### Author(s):
- Ward Whitt
### Published:
- 1 Feb 1999
### Abstract:
This paper investigates the effect upon performance in a service system, such as a telephone call center, of giving waiting customers state information. In particular, the paper studies two M/M/s/r queueing models with balking and reneging. For simplicity, it is assumed that each customer is willing to wait a fixed time before beginning service. However, customers differ, so the delay tolerances for successive customers are random. In particular, it is assumed that the delay tolerance of each customer is zero with probability , and is exponentially distributed with mean 1 conditional on the delay tolerance being positive. Let N be the number of customers found by an arrival. In Model 1, no state information is provided, so that if N  s, the customer balks with probability ; if the customer enters the system, he reneges after an exponentially distributed time with mean 1 if he has not begun service by that time. In Model 2, if N = s + k  s, then the customer is told the system state k and the remaining service times of all customers in the system, so that he balks with probability  + (1  )(1  qk), where qk = P(T > Sk), T is exponentially distributed with mean 1, Sk is the sum of k + 1 independent exponential random variables each with mean (s)1, and 1 is the mean service time. In Model 2, all reneging is replaced by balking. The number of customers in the system for Model 1 is shown to be larger than that for Model 2 in the likelihood-ratio stochastic ordering. Thus, customers are more likely to be blocked in Model 1 and are more likely to be served without waiting in Model 2. Algorithms are also developed for computing important performance measures in these, and more general, birth-and-death models.
### Link:
- https://doi.org/10.1287/mnsc.45.2.192

## 6. Correlations and Copulas for Decision and Risk Analysis
### Author(s):
- Robert T. Clemen
- Terence Reilly
### Published:
- 1 Feb 1999
### Abstract:
The construction of a probabilistic model is a key step in most decision and risk analyses. Typically this is done by defining a joint distribution in terms of marginal and conditional distributions for the model's random variables. We describe an alternative approach that uses a copula to construct joint distributions and pairwise correlations to incorporate dependence among the variables. The approach is designed specifically to permit the use of an expert's subjective judgments of marginal distributions and correlations. The copula that underlies the multivariate normal distribution provides the basis for modeling dependence, but arbitrary marginals are allowed. We discuss how correlations can be assessed using techniques that are familiar to decision analysts, and we report the results of an empirical study of the accuracy of the assessment methods. The approach is demonstrated in the context of a simple example, including a study of the sensitivity of the results to the assessed correlations.
### Link:
- https://doi.org/10.1287/mnsc.45.2.208

## 7. A Quantile Regression Approach to Generating Prediction Intervals
### Author(s):
- James W. Taylor
- Derek W. Bunn
### Published:
- 1 Feb 1999
### Abstract:
Exponential smoothing methods do not involve a formal procedure for identifying the underlying data generating process. The issue is then whether prediction intervals should be estimated by a theoretical approach, with the assumption that the method is optimal in some sense, or by an empirical procedure. In this paper we present an alternative hybrid approach which applies quantile regression to the empirical fit errors to produce forecast error quantile models. These models are functions of the lead time, as suggested by the theoretical variance expressions. In addition to avoiding the optimality assumption, the method is nonparametric, so there is no need for the common normality assumption. Application of the new approach to simple, Holt's, and damped Holt's exponential smoothing, using simulated and real data sets, gave encouraging results.
### Link:
- https://doi.org/10.1287/mnsc.45.2.225

## 8. Design of Communication Networks with Survivability Constraints
### Author(s):
- Young-Soo Myung
- Hyun-joon Kim
- Dong-wan Tcha
### Published:
- 1 Feb 1999
### Abstract:
The rapid growth of telecommunication capacity, driven in part by the wide-ranging deployment of fiber-optic technology has led to increasing concern regarding the survivability of such networks. In communication networks, survivability is usually defined as the percentage of total traffic surviving some network failures in the worst case. Most of the survivable network design models proposed to date indirectly ensure network survivability by invoking a connectivity constraint, which calls for a prespecified number of paths between every distinct pair of nodes in the network. In this paper, we introduce a new network design model which directly addresses survivability in terms of a survivability constraint which specifies the allowable level of lost traffic during a network failure under prescribed conditions. The new model enables a network designer to consider a richer set of alternative network topologies than the existing connectivity models, and encompasses the connectivity models as special cases. The paper presents a procedure to compute link survivability, develops an integer programming formulation of the proposed survivability model, and discusses a special case of practical interest and its associated heuristic procedure. The proposed heuristic is tested on data from real-world problems as well as randomly generated problems.
### Link:
- https://doi.org/10.1287/mnsc.45.2.238

## 9. Dynamic Routing and Operational Controls in Workflow Management Systems
### Author(s):
- Akhil Kumar
- J. Leon Zhao
### Published:
- 1 Feb 1999
### Abstract:
Businesses around the world are paying more attention to process management and process automation to improve organizational efficiency and effectiveness. In this paper, we describe a general framework for implementing dynamic routing and operational control mechanisms in Workflow Management Systems (WMSs). The framework consists of three techniques: workflow control tables, sequence constraints, and event-based workflow management rules. Our approach offers several unique features that are missing in commercial workflow management systems: (1) it provides more flexibility in process modeling and control; (2) it permits rework on an ad hoc basis; (3) it handles exceptions to routing and operational controls; and (4) it exploits parallelism to increase system throughput and response time. Finally, the workflow management techniques are applied to the case of consumer loan management and compared with other approaches based on static routing.
### Link:
- https://doi.org/10.1287/mnsc.45.2.253

## 10. Lotsizing and Scheduling on Parallel Machines with Sequence-Dependent Setup Costs
### Author(s):
- Sungmin Kang
- Kavindra Malik
- L. Joseph Thomas
### Published:
- 1 Feb 1999
### Abstract:
Industrial lotsizing and scheduling pose very difficult analytical problems. We propose an unconventional model that deals with sequence-dependent setup costs in a multiple-machine environment. The sequence-splitting model splits an entire schedule into subsequences, leading to tractable subproblems. An optimization approach based on a column generation/branch and bound methodology is developed, and heuristically adapted to test problems including five real-world problem instances gathered from industry.
### Link:
- https://doi.org/10.1287/mnsc.45.2.273

## 11. Optimal Lead Time Policies
### Author(s):
- Mark L. Spearman
- Rachel Q. Zhang
### Published:
- 1 Feb 1999
### Abstract:
This paper examines two due-date setting problems first studied by Wein (1991). The first problem seeks to minimize the average due-date lead time (due-date minus arrival date) of jobs subject to a constraint on the fraction of tardy jobs (Problem I) while the second uses the same objective subject to a constraint on average job tardiness (Problem II). We show that under very general conditions, Problem I leads to unethical practice (i.e., quote lead times for which there is no hope to achieve when the system is highly congested) while Problem II results in policies that quote lead times that are monotonically increasing with the congestion level. Furthermore, we prove that Problem II is equivalent to a policy that is widely used and is easy to compute. This policy quotes lead times that guarantee the same serviceability level (the fraction of tardy jobs) to all jobs.
### Link:
- https://doi.org/10.1287/mnsc.45.2.290


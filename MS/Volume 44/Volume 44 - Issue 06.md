# Volume 44, Issue 06
- June 1998
- Pages 743-877
- David Simchi-Levi

## 1. Managing Experimentation in the Design of New Products
### Author(s):
- Stefan H. Thomke
### Published:
- 1 Jun 1998
### Abstract:
Experimentation, a form of problem-solving, is a fundamental innovation activity and accounts for a significant part of total innovation cost and time. In many fields, the economics of experimentation are being radically affected by the use of new and greatly improved versions of methods such as computer simulation, mass screening, and rapid prototyping. This paper shows that a given experiment (and the related trial and error learning) can be conducted in different modes (e.g., computer simulation and rapid prototyping) and that users will find it economical to optimize the switching between these modes as to reduce total product development cost and time. The findings are confirmed by a large-scale empirical study of the experimentation process in the design of integrated circuits containing either (1) electrically programmable logic devices (EPLDs); or (2) application-specific integrated circuits (ASICs). In comparing their different experimentation strategies for analogous design projects, I found that the former (EPLD)an approach that utilizes many prototype iterationsoutperformed the latter (ASIC) by factor of 2.2 (in person-months) and over 43 percent of that difference can be attributed to differences in experimentation strategies. The implications for managerial practice and theory are discussed and suggestions for further research undertakings are provided.
### Link:
- https://doi.org/10.1287/mnsc.44.6.743

## 2. An Analysis of Product Lifetimes in a Technologically Dynamic Industry
### Author(s):
- Barry L. Bayus
### Published:
- 1 Jun 1998
### Abstract:
The conventional wisdom that product lifetimes are shrinking has important implications for technology management and product planning. However, very limited empirical information on this topic is available. In this paper, product lifetimes are directly measured as the time between product introduction and withdrawal. Statistical analyses of desktop personal computer models introduced between 1974 and 1992 are conducted at various product market levels. Results indicate that (1) product technology and product model lifetimes have not accelerated, and (2) manufacturers have not systematically reduced the life-cycles of products within their lines. Instead, the products of firms that have entered this industry in the more recent years tend to be based on previously existing technology, and, not surprisingly, these products have lifetimes that are shorter than those of established firms. Implications of these findings are discussed.
### Link:
- https://doi.org/10.1287/mnsc.44.6.763

## 3. Modifying Customer Expectations of Price Decreases for a Durable Product
### Author(s):
- Subramanian Balachander
- Kannan Srinivasan
### Published:
- 1 Jun 1998
### Abstract:
We study the introductory signalling strategy for a durable product that faces optimistic expectations among customers about price declines over time. The firm introducing the product knows that experiential learning is low for the product. However, customers, being uncertain about the extent of experiential learning, assign a nonzero probability that the firm's new product will enjoy a high cost reduction with cumulative experience. The optimistic expectations of customers reduce their willingness to pay a high price at the product's introduction while predisposing them to buying later. The challenge facing the low-experience firm is to choose an introductory strategy that will credibly convey the low experience-curve effect to customers. We use the sequential equilibrium concept in a game-theoretic framework to identify the firm's signalling strategy. We identify the unique separating equilibrium of the game after refining the set of separating equilibria. We demonstrate that a high introductory price credibly signals the low experiential learning to customers. We also show that signalling causes an artificial learning-curve effect.
### Link:
- https://doi.org/10.1287/mnsc.44.6.776

## 4. The Missing Link in Budget Models of Nonprofit Institutions: Two Practical Dutch Applications
### Author(s):
- Piet Verheyen
### Published:
- 1 Jun 1998
### Abstract:
The Dutch government, health insurance associations, and research boards allocate budgets to nonprofit institutions (hospitals, universities), based on general indicators (inhabitants, admissions, or students). With these budgets, the top management of each institution (the principal) allocates the funds, using general indicators to distribute the money further down to the base. At the base of the nonprofit institutions, the professionals (the agents) perform a spectrum of specific tasks. Tension around the budgetting process exists between top management and professionals. This paper resolves the tension along the research lines of Burton and Obel (1995) by measuring in money terms the specific tasks of the professionals of universities and hospitals at the base, and by reconsidering the theoretical background of the internal budget system. The study also develops the missing link in an integral external and internal budget system by integrating the managerial and professional decisions or the input-output decisions in one approach.
### Link:
- https://doi.org/10.1287/mnsc.44.6.787

## 5. Differences in Subjective Risk Thresholds: Worker Groups as an Example
### Author(s):
- Anil Gaba
- W. Kip Viscusi
### Published:
- 1 Jun 1998
### Abstract:
Subjective risk perceptions are often encoded as responses to 0-1 questions in surveys or other qualitative risk scales. However, reference points for assessing an activity as risky are confounded by various characteristics of the respondents. This paper uses a sample of workers for whom quantitative risk assessments as well as dichotomous risk perception responses are available. It is shown that, given a quantitative risk measure, the thresholds for assessing an activity as risky vary systematically, particularly by education. The differences in such thresholds across worker groups are estimated. The resulting implications of using qualitative risk variables for assessing wage-risk tradeoffs are estimated, yielding results that are also relevant for many other areas involving similar qualitative variables.
### Link:
- https://doi.org/10.1287/mnsc.44.6.801

## 6. Validation of Trace-Driven Simulation Models: A Novel Regression Test
### Author(s):
- Jack P. C. Kleijnen
- Bert Bettonvil
- Willem Van Groenendaal
### Published:
- 1 Jun 1998
### Abstract:
This paper argues that it is wrong to require that regressing the outputs of a trace-driven simulation on the observed real outcomes should give a 45 (unit slope) line through the origin (zero intercept). This note proposes instead an alternative requirement: the responses of the simulated and the real systems should have the same means and the same variances. To test statistically whether this requirement is satisfied, a novel procedure is derived: regress the differences between simulated and real responses on their associated sums, and test whether the resulting intercept and slope are both zero. This novel but simple test assumes identically, independently, and normally distributed outputs of the real system and the simulated system. The old and the new procedures are investigated in extensive Monte Carlo experiments that simulate M/M/1 queueing systems. The conclusions are: (i) the naive intuitive test rejects a valid simulation model substantially more often than the novel test does; (ii) the naive test shows perverse behavior within a certain domain: the worse the simulation model, the higher its estimated probability of acceptance; and (iii) the novel test does not reject a valid simulation model too often (its type I error probability is correct), provided the queueing response is transformed appropriately to obtain (nearly) normally distributed responses.
### Link:
- https://doi.org/10.1287/mnsc.44.6.812

## 7. Active Nonlinear Tests (ANTs) of Complex Simulation Models
### Author(s):
- John H. Miller
### Published:
- 1 Jun 1998
### Abstract:
Simulation models are becoming increasingly common in the analysis of critical scientific, policy, and management issues. Such models provide a way to analyze complex systems characterized by both large parameter spaces and nonlinear interactions. Unfortunately, these same characteristics make understanding such models using traditional testing techniques extremely difficult. Here we show how a model's structure and robustness can be validated via a simple, automatic, nonlinear search algorithm designed to actively break the model's implications. Using the active nonlinear tests (ANTs) developed here, one can easily probe for key weaknesses in a simulation's structure, and thereby begin to improve and refine its design. We demonstrate ANTs by testing a well-known model of global dynamics (World3), and show how this technique can be used to uncover small, but powerful, nonlinear effects that may highlight vulnerabilities in the original model.
### Link:
- https://doi.org/10.1287/mnsc.44.6.820

## 8. An Algorithm for Single-Item Capacitated Economic Lot Sizing with Piecewise Linear Production Costs and General Holding Costs
### Author(s):
- Dong X. Shaw
- Albert P. M. Wagelmans
### Published:
- 1 Jun 1998
### Abstract:
We consider the Capacitated Economic Lot Size Problem with piecewise linear production costs and general holding costs, which is an NP-hard problem but solvable in pseudo-polynomial time. A straightforward dynamic programming approach to this problem results in an O(n2cd) algorithm, where n is the number of periods, and d and c are the average demand and the average production capacity over the n periods, respectively. However, we present a dynamic programming procedure with complexity O(n2qd), where q is the average number of pieces required to represent the production cost functions. In particular, this means that problems in which the production functions consist of a fixed set-up cost plus a linear variable cost are solved in O(n2d) time. Hence, the running time of our algorithm is only linearly dependent on the magnitude of the data. This result also holds if extensions such as backlogging and startup costs are considered. Moreover, computational experiments indicate that the algorithm is capable of solving quite large problem instances within a reasonable amount of time. For example, the average time needed to solve test instances with 96 periods, 8 pieces in every production cost function, and average demand of 100 units is approximately 40 seconds on a SUN SPARC 5 workstation.
### Link:
- https://doi.org/10.1287/mnsc.44.6.831

## 9. The Zero-Condition: A Simplifying Assumption in QALY Measurement and Multiattribute Utility
### Author(s):
- John M. Miyamoto
- Peter P. Wakker
- Han Bleichrodt
- Hans J. M. Peters
### Published:
- 1 Jun 1998
### Abstract:
This paper studies the implications of the zero-condition for multiattribute utility theory. The zero-condition simplifies the measurement and derivation of the Quality Adjusted Life Year (QALY) measure commonly used in medical decision analysis. For general multiattribute utility theory, no simple condition has heretofore been found to characterize multiplicatively decomposable forms. When the zero-condition is satisfied, however, such a simple condition, standard gamble invariance, becomes available.
### Link:
- https://doi.org/10.1287/mnsc.44.6.839

## 10. Externalities, Tangible Externalities, and Queue Disciplines
### Author(s):
- Moshe Haviv
- Ya'acov Ritov
### Published:
- 1 Jun 1998
### Abstract:
Externalities are the (marginal) costs that a user of a common resource imposes on others. We introduce the efficient measure of tangible externalities that are the costs that a user imposes on others while being served. Then, for a single server queueing system under various service disciplines, we compute the expected externalities and the expected tangible externalities conditioning on the length of the service requirement.
### Link:
- https://doi.org/10.1287/mnsc.44.6.850

## 11. Lot Sizing in General Assembly Systems with Setup Costs, Setup Times, and Multiple Constrained Resources
### Author(s):
- Elena Katok
- Holly S. Lewis
- Terry P. Harrison
### Published:
- 1 Jun 1998
### Abstract:
We introduce a heuristic method for finding good, feasible solutions for multiproduct lot sizing problems with general assembly structures, multiple constrained resources, and nonzero setup costs and setup times. We evaluate the performance of this heuristic by comparing its solutions to optimal solutions of small randomly generated problems and to time-truncated Optimization Subroutine Library (OSL) solutions of medium-sized randomly generated problems. In the first case, the heuristic locates solutions averaging 4 percent worse than optimal in less than 1 percent of time required by OSL. The heuristic solutions to medium-sized problems are approximately 26 percent better than solutions OSL finds after 10,000 CPU seconds, and the heuristic finds these solutions in approximately 10 percent of OSL time.
### Link:
- https://doi.org/10.1287/mnsc.44.6.859


# Volume 40, Issue 05
- May 1994
- Pages 549-684
- David Simchi-Levi

## 1. Determinants of Electronic Integration in the Insurance Industry: An Empirical Test
### Author(s):
- Akbar Zaheer
- N. Venkatraman
### Published:
- 1 May 1994
### Abstract:
Electronic integrationa form of vertical quasi-integration achieved through the deployment of dedicated computers and communication systems between relevant actors in the adjacent stages of the value-chainis an important concept to researchers in the information systems field since it focuses on the role of information technology in restructuring vertical relationships. Drawing on theoretical and empirical research on transaction costs, we develop and test a model of the determinants of the degree of electronic integration in the commercial segment of the property and casualty (P&C) industry. Based on a sample of 120 independent agencies operating under dedicated information technology-mediated conditions, we provide empirical support for three hypotheses on the determinants of electronic integration. Implications and research extensions are identified to guide further research in this important area.
### Link:
- https://doi.org/10.1287/mnsc.40.5.549

## 2. A Dual-Based Algorithm for Multi-Level Network Design
### Author(s):
- Anantaram Balakrishnan
- Thomas L. Magnanti
- Prakash Mirchandani
### Published:
- 1 May 1994
### Abstract:
Given an undirected network with L possible facility types for each edge, and a partition of the nodes into L levels or grades, the Multi-level Network Design (MLND) problem seeks a fixed cost minimizing design that spans all the nodes and connects the nodes at each level by facilities of the corresponding or higher grade. This problem generalizes the well-known Steiner network problem and the hierarchical network design problem, and has applications in telecommunication, transportation, and electric power distribution network design. In a companion paper we studied alternative model formulations for a two-level version of this problem, and analyzed the worst-case performance of several heuristics based on Steiner network and spanning tree solutions. This paper develops a dual-based algorithm for the MLND problem. The method first performs problem preprocessing to fix certain design variables, and then applies a dual ascent procedure to generate upper and lower bounds on the optimal value. We report extensive computational results on large, random two-level test problems (containing up to 500 nodes, and 5,000 edges) with varying cost structures. The integer programming formulation of the largest of these problems has 20,000 integer variables and over 5 million constraints. Our tests indicate that the dual-based algorithm is very effective, producing solutions that are within 0.9% of optimality.
### Link:
- https://doi.org/10.1287/mnsc.40.5.567

## 3. Optimizing Inventory Levels in a Two-Echelon Retailer System with Partial Lost Sales
### Author(s):
- Steven Nahmias
- Stephen A. Smith
### Published:
- 1 May 1994
### Abstract:
This paper considers a retailer inventory system with N first-echelon stores and a single second-echelon distribution center (DC). Customer demands at the stores are assumed to be random. When a stockout occurs, customers are willing to wait for their order to be filled with a known probability. Customers who are unwilling to wait result in lost sales. The first and second echelons are both restocked at fixed, equally spaced time points, where the store restocking frequency is an integer multiple of the DC restocking frequency. We also assume that replenishment quantities at both echelons can be adjusted up to the time of delivery, resulting in replenishment lead times equal to zero. This simplification allows us to determine optimal solutions for the partial lost sales case, which has proven intractable for two-echelon formulations with lead times. Computational results are given for illustrative examples.
### Link:
- https://doi.org/10.1287/mnsc.40.5.582

## 4. Multi-Echelon vs. Single-Echelon Inventory Control Policies for Low-Demand Items
### Author(s):
- Warren H. Hausman
- Nesim K. Erkip
### Published:
- 1 May 1994
### Abstract:
Multi-echelon inventory systems are often controlled as a network of single-echelon inventory systems for simplicity of managerial authority, organizational control, and performance monitoring. This paper explores the amount of suboptimization in such a situation, using an actual demand data set provided by other researchers. We consider low-demand, high-cost items controlled on an (S  1, S) basis, with all warehouse stockouts met on an emergency-ordering basis. We demonstrate that the suboptimality penalty for this data set is 3% to 5% when single-echelon systems are appropriately parameterized.
### Link:
- https://doi.org/10.1287/mnsc.40.5.597

## 5. The Effect of Leadtime Uncertainty in a Simple Stochastic Inventory Model
### Author(s):
- Jing-Sheng Song
### Published:
- 1 May 1994
### Abstract:
We study a basic continuous-time single-item inventory model where demands form a compound Poisson process and leadtimes are stochastic. The performance measure of interest is the long-run average cost. Order costs are linear, so a base-stock policy is optimal. We focus on the behavior of the optimal base-stock level in response to stochastically larger or more variable leadtimes. We also investigate the behavior of the corresponding long-run average costs. We show that a stochastically larger leadtime requires a higher optimal base-stock level. However, a stochastically larger leadtime may not necessarily result in a higher optimal average cost, because sometimes the variability effects may dominate. On the other hand, a more variable leadtime always leads to a higher optimal average cost. The effect of leadtime variability on optimal policies depends on the inventory cost structure: A more variable leadtime requires a higher optimal base-stock level if and only if the unit penalty (holding) cost rate is high (low).
### Link:
- https://doi.org/10.1287/mnsc.40.5.603

## 6. Decision Dynamics in Two High Reliability Military Organizations
### Author(s):
- Karlene H. Roberts
- Suzanne K. Stout
- Jennifer J. Halpern
### Published:
- 1 May 1994
### Abstract:
In this research we extend theoretical development about decision making in organizations in which many kinds of errors cannot be tolerated. Catastrophic consequences can be associated with faulty decision making in reliability-seeking organizations, a situation which does not occur in most organizations studied in the past. Observations are drawn from two nuclear-powered aircraft carriers. We find decision processes which appear to change often in these organizations. Important decisions can be made by a number of men even at the lowest levels of the organization. Task-related factors such as technical complexity, high interdependence, and catastrophic consequences associated with rare events and more cognitive factors such as accountability and salience affect decision processes. A model is presented that accounts for dynamic change in decision processes in these organizations.
### Link:
- https://doi.org/10.1287/mnsc.40.5.614

## 7. Folding Back in Decision Tree Analysis
### Author(s):
- Rakesh Sarin
- Peter Wakker
### Published:
- 1 May 1994
### Abstract:
This note demonstrates that two minimal requirements of decision tree analysis, the folding back procedure and the interchangeability of consecutive event nodes, imply independence.
### Link:
- https://doi.org/10.1287/mnsc.40.5.625

## 8. Optimal Leadtimes Planning in a Serial Production System
### Author(s):
- Linguo Gong
- Ton de Kok
- Jie Ding
### Published:
- 1 May 1994
### Abstract:
Consider an N stage serial production line where the processing times of orders may be random. Since the carrying costs increase from stage to stage, the standard production procedure, that is, to determine a total leadtime for the entire order by taking an appropriate percentile of the distribution of total processing time and then release the order immediately from stage to stage during the process, may not be optimal since it ignores inventory carrying costs. This article studies a per stage planned leadtime dispatching policy for such systems. The order will not be released immediately to the next workstation prior to a predetermined delivery time, or planned leadtime. The vector of planned leadtimes at workstations is to be determined by trading off expected holding costs at all stages and expected penalty costs for exceeding the total planned leadtime. We show that the optimal vector of planned leadtimes may be obtained efficiently by solving an equivalent serial inventory model of the type considered in Clark and Scarf (1960).
### Link:
- https://doi.org/10.1287/mnsc.40.5.629

## 9. Pricing and Delivery-Time Performance in a Competitive Environment
### Author(s):
- Lode Li
- Yew Sing Lee
### Published:
- 1 May 1994
### Abstract:
We present a model of market competition in which customer preferences are over not only price and quality but also delivery speed. This allows a study of market demand and firms' decisions on price, quality, technology and responsiveness in a competitive environment. When demand arises, a customer chooses the firm that maximizes its expected utility of price, quality and response time. The demand function for each firm is derived by analyzing a queueing system with competing servers. We then study price competition among firms with differentiated processing rates. In the equilibrium, the firm with a higher processing rate always enjoys a price premium, and, further, enjoys a larger market share when its opponent also has adequate processing rate to serve all the customers alone.
### Link:
- https://doi.org/10.1287/mnsc.40.5.633

## 10. A Network Model to Maximize Navy Personnel Readiness and Its Solution
### Author(s):
- Iosif A. Krass
- Mustafa . Pinar
- Theodore J. Thompson
- Stavros A. Zenios
### Published:
- 1 May 1994
### Abstract:
The problem of optimally (re)allocating Navy personnel to combat units is compounded by several considerations: availability of trained personnel, staffing of positions by occupation groups or ranks, and maintaining an acceptable level of readiness. In this paper we model this problem as a nonlinear nondifferentiable optimization problem. A reformulation of the nonlinear optimization problem as a network flow problem is then developed. The formulation results in a network flow problem with side constraints. An additional, nonnetwork, variable measures the readiness level. This new formulation permits the use of network optimization tools in order to solve effectively very large problems.
### Link:
- https://doi.org/10.1287/mnsc.40.5.647

## 11. The Generating Process and an Extension of Jewitt's Location Independent Risk Concept
### Author(s):
- Michael Landsberger
- Isaac Meilijson
### Published:
- 1 May 1994
### Abstract:
A generating process of Jewitt's location independent risk concept is derived in terms of left stretches based on single crossings between distributions. For concave nondecreasing utility functions this stochastic order preserves monotonicity between risk premium and the Arrow-Pratt measure of risk aversion. We show that a stronger order, the Bickel-Lehmann notion of dispersion, preserves this monotonicity for the larger class of nondecreasing utilities.
### Link:
- https://doi.org/10.1287/mnsc.40.5.662

## 12. Marginal Conditional Stochastic Dominance
### Author(s):
- Haim Shalit
- Shlomo Yitzhaki
### Published:
- 1 May 1994
### Abstract:
This paper introduces the concept of Marginal Conditional Stochastic Dominance (MCSD), which states the conditions under which all risk-averse individuals, when presented with a given portfolio, prefer to increase the share of one risky asset over that of another. MCSD rules also answer the question of whether all risk-averse individuals include a new asset in their portfolio when assets' returns are correlated. MCSD criteria are expressed in terms of the probability distributions of the assets and of the underlying portfolio. An empirical application of MCSD is provided using stocks traded on the New York Stock Exchange. MCSD rules are used to show that, in the long run, one cannot assert that the market portfolio is inefficient.
### Link:
- https://doi.org/10.1287/mnsc.40.5.670


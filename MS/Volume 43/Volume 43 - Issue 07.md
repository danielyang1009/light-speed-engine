# Volume 43, Issue 07
- July 1997
- Pages 895-1045
- David Simchi-Levi

## 1. Making a Case for Robust Optimization Models
### Author(s):
- Dawei Bai
- Tamra Carpenter
- John Mulvey
### Published:
- 1 Jul 1997
### Abstract:
Robust optimization searches for recommendations that are relatively immune to anticipated uncertainty in the problem parameters. Stochasticities are addressed via a set of discrete scenarios. This paper presents applications in which the traditional stochastic linear program fails to identify a robust solutiondespite the presence of a cheap robust point. Limitations of piecewise linearization are discussed. We argue that a concave utility function should be incorporated in a model whenever the decision maker is risk averse. Examples are taken from telecommunications and financial planning.
### Link:
- https://doi.org/10.1287/mnsc.43.7.895

## 2. Tabu Search and Ejection ChainsApplication to a Node Weighted Version of the Cardinality-Constrained TSP
### Author(s):
- Buyang Cao
- Fred Glover
### Published:
- 1 Jul 1997
### Abstract:
A cardinality-constrained TSP (CC-TSP) problem requires the salesman to visit at least L and at most U cities, represented by nodes of a graph. The objective of this problem is to maximize the sum of weights of nodes visited. In this paper we propose a tabu search method based on ejection chain procedures, which have proved effective for many kinds of combinatorial optimization problems. Computational results on a set of randomly generated test problems with various implementations of the algorithm are reported.
### Link:
- https://doi.org/10.1287/mnsc.43.7.908

## 3. Garbage Collection in Chicago: A Dynamic Scheduling Model
### Author(s):
- Donald D. Eisenstein
- Ananth. V. Iyer
### Published:
- 1 Jul 1997
### Abstract:
We investigate the scheduling of garbage trucks in the city of Chicago. Analysis of data collected from the system shows that city blocks differ in the rate at which garbage is collected. However, in the current system, each truck visits the dumpsite two times each day. Our approach is to devise a flexible routing scheme in which some routes visit the dumpsite only once per day, while others visit the dumpsite twice per day depending on the blocks assigned to the route. We use a Markov decision process to model the impact on capacity of using flexible routes. This provides a dynamic scheduling algorithm that adjusts the number of dumpsite visits throughout the week to maximize service level. Results of the model suggest a potential reduction in truck capacity of 1216% for a set of five pilot wards. This paper shows that flexible schedules can significantly reduce the capacity required to operate a system in the presence of variability.
### Link:
- https://doi.org/10.1287/mnsc.43.7.922

## 4. Adaptation on Rugged Landscapes
### Author(s):
- Daniel A. Levinthal
### Published:
- 1 Jul 1997
### Abstract:
A simple model is developed to explore the interrelationship between processes of organizational level change and population selection forces. A critical property of the model is that the effect on organizational fitness of the various attributes that constitute an organization's form is interactive. As a result of these interaction effects, the fitness landscape is rugged. An organization's form at founding has a persistent effect on its future form when there are multiple peaks in the fitness landscape, since the particular peak that an organization discovers is influenced by its starting position in the space of alternative organizational forms. Selection pressures influence the distribution of the organizational forms that emerge from the process of local adaptation. The ability of established organizations to respond to changing environments is importantly conditioned by the extent to which elements of organizational form interact in their effect on organizational fitness. Tightly coupled organizations are subject to high rates of failure in changing environments. Furthermore, successful reorientations are strongly associated with survival for tightly coupled organizations, but not for more loosely coupled organizations that are able to engage in effective local adaptation.
### Link:
- https://doi.org/10.1287/mnsc.43.7.934

## 5. Tractable (Q, R) Heuristic Models for Constrained Service Levels
### Author(s):
- David E. Platt
- Lawrence W. Robinson
- Robert B. Freund
### Published:
- 1 Jul 1997
### Abstract:
The fill rate (the proportion of demand that is satisfied from stock) is a viable alternative in inventory models to the hard-to-quantify penalty cost. However, a number of difficulties have impeded its implementation, among them that the existing cycle-based approximate solutions do not reflect the possibility of multiple outstanding orders and that the optimal policy cannot be found directly, but must be iteratively calculated. We show that for a large family of leadtime demand distributions, the optimal policy depends on only two parameters: the fill rate and the economic order quantity (EOQ) scaled by the standard deviation of demand over the constant leadtime. If we then assume that the leadtime demand is normally distributed, we can use the asymptotic results as the EOQ goes to zero and to positive infinity to fit atheoretic curves for the order quantity Q and the reorder point R. These fitted curves yield a good (Q, R) policy without iteration. We also find that, among the set of simple heuristics, the limit form as EOQ goes to positive infinity provides a better alternative to simply setting Q equal to the EOQ.
### Link:
- https://doi.org/10.1287/mnsc.43.7.951

## 6. Calculating the Reserve for a Time and Usage Indexed Warranty
### Author(s):
- Jehoshua Eliashberg
- Nozer D. Singpurwalla
- Simon P. Wilson
### Published:
- 1 Jul 1997
### Abstract:
Many products carry a warranty that offers protection for the consumer against low quality. These warranties are often two dimensional, such as an automobile warranty that guarantees repair up to a certain time and mileage after sale. This paper considers the problem of assessing the size of a reserve needed by the manufacturer to meet future claims for such a two-dimensional warranty. To do this, a class of failure models that describe failure by two scalestime and mileage, for examplemust be developed. The first half of the paper is devoted to this development. Then the warranty reserve problem is considered in more detail. The problem is described and a decision-theoretic solution, making use of the newly developed reliability model, is proposed.
### Link:
- https://doi.org/10.1287/mnsc.43.7.966

## 7. A Theoretical Study of Organizational Performance Under Information Distortion
### Author(s):
- Kathleen M. Carley
- Zhiang Lin
### Published:
- 1 Jul 1997
### Abstract:
How should organizations of intelligent agents be designed so that they exhibit high performance despite information distortion? We present a formal information-based network model of organizational performance given a distributed decision making environment in which agents encounter a radar detection task. Using this model, we examine the performance of organizations with various designs in different task environments subject to various types of information distortion. We distinguish five sources of information distortionmissing information, incorrect information, agent unavailability, communication channel breakdowns, and agent turnover. This formal analysis suggests that: (1) regardless of information distortion, performance is enhanced if there is a match between the complexity of organizational design and task environment; (2) task environment characteristics have more effect on performance than information distortion and the organizational design; (3) the effects of information distortion can be combated by training, but only to a limited extent; and (4) technology based information distortion typically is more debilitating than personnel induced information distortion.
### Link:
- https://doi.org/10.1287/mnsc.43.7.976

## 8. A Dynamic, Globally Diversified, Index Neutral Synthetic Asset Allocation Strategy
### Author(s):
- Frederick Novomestky
### Published:
- 1 Jul 1997
### Abstract:
An investor with the ability to assess the prospective return and risk structure of the global capital markets can construct portfolios that, over time, will not only outperform actively or passively managed domestic asset portfolios but will also outperform passively managed global portfolios. A Bayesian approach to dynamic seemingly unrelated regression (DSUR) is a robust and effective means to forecast the one-step ahead, conditional distribution of asset returns. This approach recognizes the time-varying nature of the global capital markets. The predictive moments are used to derive a single-factor return model once an index portfolio is specified. The index portfolio represents an investor's underlying portfolio. Given the factor model that assesses the relative attractiveness and risk of the assets in relation to the index, an index neutral portfolio is constructed as an overlay to enhance the returns of the index portfolio. This portfolio is mean-variance optimal, is notional neutral (i.e., the sum of the asset exposures is zero), and has returns that are designed to be uncorrelated with the returns of the index portfolio. The implementation of such an index neutral portfolio using derivative securities is simulated over the period January 1988 to December 1993.
### Link:
- https://doi.org/10.1287/mnsc.43.7.998

## 9. Using On-Line Sensors in Statistical Process Control
### Author(s):
- Linguo Gong
- Wushong Jwo
- Kwei Tang
### Published:
- 1 Jul 1997
### Abstract:
As manufacturing technology moves toward more computerized automation, statistical process control (SPC) techniques must adapt to keep pace with the new environment and take advantage of the development in automated on-line sensors. In this paper, a two-phase procedure is proposed for combining an on-line sensor and a control chart to improve statistical process control decisions. In phase 1 of this procedure, a production process is monitored continually by a sensor. When a sensor warning signal is observed, phase 2 takes place: A sample of items is drawn from the process and inspected. If the sample mean is outside the predetermined control limits, the process is stopped, and a search is initiated to determine the actual process status for possible necessary adjustment. If the sample mean is within the control limits, the process continues. A mathematical model is formulated for jointly determining the sample size and the control limit of the control chart and a decision rule for sending out sensor warning signals. The model is based on the assumption that there is only a weak relationship between the sensor measurement and the process condition. A solution algorithm based on a numerical search is developed. A numerical example is used to show the advantage of the proposed model over the models based separately on the sensor and the control chart, and a sensitivity analysis is used to show the effects of several important model parameters on the optimal solution.
### Link:
- https://doi.org/10.1287/mnsc.43.7.1017

## 10. Decomposed Versus Holistic Estimates of Effort Required for Software Writing Tasks
### Author(s):
- Terry Connolly
- Doug Dean
### Published:
- 1 Jul 1997
### Abstract:
We examine decision analysis central decomposition principle in the context of work-time estimates of software writers. Two experiments examined the abilities of advanced programming students to estimate how long they would take to complete specific software projects. They estimated their own work times both for entire projects and for their constituent subtasks. Estimates showed varying degrees of overoptimism and overpessimism but all were much too tight, with almost half of actual outcomes falling in the 1% tails of estimated distributions. This overtightness was unaffected by task decomposition, question wording, question order, or training in estimation. It was, however, significantly reduced by a procedure aimed at inducing generous upper and lower plausible limits. An underlying model of incomplete search is used to connect these findings to existing themes in cognition and judgment research, as well as to practical application. The findings suggest that the best level of decomposition at which to elicit work-time estimates may depend on task, judge, and elicitation method.
### Link:
- https://doi.org/10.1287/mnsc.43.7.1029


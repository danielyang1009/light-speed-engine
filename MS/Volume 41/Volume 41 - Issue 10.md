# Volume 41, Issue 10
- October 1995
- Pages 1565-1715
- David Simchi-Levi

## 1. The Crucial Interrelationship Between Manufacturing Strategy and Organizational Culture
### Author(s):
- Kimberly A. Bates
- Susan D. Amundson
- Roger G. Schroeder
- William T. Morris
### Published:
- 1 Oct 1995
### Abstract:
This paper proposes a relationship between manufacturing strategy and organizational culture, based on an examination of the research literature. Survey data were collected from 822 respondents in 41 plants in the transportation, electronics, and machinery industries in the U.S. These plants included random samples of U.S.-owned and Japanese-owned manufacturers in the U.S., and manufacturers reputed to use advanced manufacturing practices. Analysis indicates that manufacturing strategy and organizational culture are related, and that a manufacturer with a well-aligned and implemented manufacturing strategy exhibits a collectivist or group-oriented organizational culture with coordinated decision making, decentralized authority, and a loyal work force.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1565

## 2. The Delivery and Control of Quality in Supplier-Producer Contracts
### Author(s):
- Diane J. Reyniers
- Charles S. Tapiero
### Published:
- 1 Oct 1995
### Abstract:
We model the effect of contract parameters such as price rebates and after-sales warranty costs on the choice of quality by a supplier, the inspection policy of a producer, and the resulting end product quality. Both noncooperative and cooperative settings are explored. The papers contribution is to highlight the importance of strategic and contractual issues in quality management.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1581

## 3. Minimizing Resource Availability Costs in Time-Limited Project Networks
### Author(s):
- Erik Demeulemeester
### Published:
- 1 Oct 1995
### Abstract:
We consider the problem of minimizing renewable resource availability costs in an activity-on-the-node project network subject to a project due date. Project activities have fixed durations and may require the use of multiple renewable resources in constant amounts throughout their duration. Various assumptions may be made about the type of precedence relations, ready times, due dates, and task interruptability. Given a discrete, non-decreasing cost function of the constant resource availability for every resource type, the objective is to determine the resource availability levels in order to minimize the sum of the availability costs over all resource types. An effective optimal algorithm is described and extensive computational experience is reported.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1590

## 4. Customer-Order Information, Leadtimes, and Inventories
### Author(s):
- Rema Hariharan
- Paul Zipkin
### Published:
- 1 Oct 1995
### Abstract:
We have an inventory to manage. The scenario is standard in all respects save one. Instead of arriving unannounced, customers provide advance warning of their demands. How should we use this information, and what is its effect on system performance? The answers turn out to be strikingly simple. There are very simple policies which perform effectively, in some cases optimally. Also such demand leadtimes improve performance, in precisely the same way that replenishment leadtimes degrade it.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1599

## 5. Demand Signalling Under Unobservable Effort in Franchising: Linear and Nonlinear Price Contracts
### Author(s):
- Preyas S. Desai
- Kannan Srinivasan
### Published:
- 1 Oct 1995
### Abstract:
We study the signalling strategy of a principal who is privately informed about its high demand potential to an uninformed risk-neutral agent. We analyze the model in the context of a contract between a franchisor and a franchisee. We examine the distortions of a two-part pricing scheme necessary to credibly inform the franchisee (agent). We also study whether the inability of the franchisor (principal) to observe the agents effort moderates or exaggerates the distortions from the first-best two-part pricing scheme. A surprising outcome is that even though the principal incurs greater signalling cost, the magnitude of distortion in the two-part scheme is smaller when service is unobservable than when it is not. Thus, a signalling strategy employing the fixed and variable fees is harder to detect under moral hazard. Empirical studies failing to control for moral hazard may incorrectly conclude that signalling strategy does not occur. We later consider a three-part scheme to verify whether or not the scheme reduces signalling cost. Interestingly, we find that there exists a unique three-part scheme that results in the first-best profit even in the presence of two-sided information asymmetries. While the two-part scheme can never achieve the first-best profit, the three-part scheme always achieves the first-best profit. The costless three-part separating scheme relies on variable income to induce the agent to undertake first-best service. The reliance on variable income to alleviate moral hazard contrasts with the two-part schemes focus on reducing the variable income to overcome the inability to monitor. The finding provides additional empirical implications.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1608

## 6. Technical Dialog as an Incentive for Vertical Integration in the Semiconductor Industry
### Author(s):
- Kirk Monteverde
### Published:
- 1 Oct 1995
### Abstract:
A curious structural change observed in the American semiconductor industry (the appearance of a class of fabless firms) is examined within a general transaction costs framework. The framework is refined for empirical application with the introduction of the construct unstructured technical dialog. The necessary level of this form of interpersonal communication between engineers in the design and fabrication stages of production is hypothesized to be positively related to the efficiency of a vertically integrated structure in which both stages are organized under a single hierarchy Fablessness (i.e., the organizational separation of the design and fabrication stages) is argued to be efficient for only those firms whose particular product portfolios have only a minimal requirement for such unstructured technical dialog. The hypothesis is tested upon a set of relatively young semiconductor firms. Support is found for the theoretically derived relationship even after controlling for rival explanations found in the literature.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1624

## 7. Belief Assessment: An Underdeveloped Phase of Probability Elicitation
### Author(s):
- P. George Benson
- Shawn P. Curley
- Gerald F. Smith
### Published:
- 1 Oct 1995
### Abstract:
A cognitive analysis of subjective probability is applied to the evaluation of techniques used by decision analysts for eliciting probabilities from experts. The construction of a subjective probability requires both the formation of a belief and the assessment of a probability that qualifies the belief. The former process involves judgment and reasoning; the latter is purely judgmental. Subjective probabilities have traditionally been portrayed and studied as arising from judgment. Consequently, belief assessment procedures have been particularly underdeveloped. Procedures currently used by analysts to and belief assessment are classified and evaluated. Although such procedures facilitate the communication of beliefs and offer important guidance for constructing probabilities, additional prescriptive development is possible. It is argued that significant improvements in assessment practice can be realized by providing better support for the reasoning employed by experts in belief assessment. Opportunities for descriptive and prescriptive research in belief assessment are identified.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1639

## 8. Forest Management: A Multicommodity Flow Formulation and Sensitivity Analysis
### Author(s):
- Antoine Gautier
- Frieda Granot
### Published:
- 1 Oct 1995
### Abstract:
We formulate the Forest Management Problem as a Multicommodity Network Flow Problem with a convex cost function. We then show how to transfer the problem to an equivalent Single-Commodity Network Flow formulation in order to address several sensitivity analysis questions using results by Granot and Veinott (Granot, F., A. F. Veinott, Jr. 1985. Substitutes, complements and ripples in network flows. Math. Oper. Res.10 471497.). The answers can subsequently be used to help forest managers respond to changes in the environment such as fires or changes in market prices, without any additional computation. Furthermore, the characterization of the response we provide requires no prior knowledge of optimal management policies.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1654

## 9. Improving Productivity by Periodic Performance Evaluation: A Bayesian Stochastic Model
### Author(s):
- Emmanuel Fernndez-Gaucherand
- Sanjay Jain
- Hau L. Lee
- Ambar G. Rao
- M. R. Rao
### Published:
- 1 Oct 1995
### Abstract:
We model the situation where the productivity of members of a group, such as a salesforce, is periodically evaluated; those whose performance is sub-par are dismissed and replaced by new members. Individual productivity is modeled as a random variable, the distribution of which is a function of an unknown parameter. This parameter varies across the members of the group and is specified by a prior distribution. In this manner, the heterogeneity in the group is explicitly accounted for. We model the situation as a parameter adaptive Bayesian stochastic control problem, and use dynamic programming techniques and the appropriate optimality equations to obtain solutions. We prove the existence of an optimal policy in the general case. Further, for the case when the sales process can be characterized by a Beta-Binomial or a Gamma-Poisson distribution, we show that the optimal policy is of the threshold type at each evaluation period, depending only on the accumulated performance up to a given period. We present a computational procedure to solve for the optimal thresholds. Results of computational experiments show that an increase in the heterogeneity of the group can lead to more stringent levels of minimal acceptable performance.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1669

## 10. Minimal Adjustment Costs and the Optimal Choice of Inputs Under Time-of-Use Electricity Rates
### Author(s):
- Yishai Spector
- Asher Tishler
- Yinyu Ye
### Published:
- 1 Oct 1995
### Abstract:
In Time-of-Use(TOU) pricing schemes, utilities charge rates that depend on the time of day and the season of the year at which electricity is used. Estimates of the effects of TOU rates on business customers in the U.S. and Israel demonstrate that most firms do not appear to respond at all to newly introduced TOU rates, but those that do respond make substantial adjustments. In this paper we show that adjustment costs associated with changing the level of employment can explain the observed pattern of behavior in the U.S. and Israel. We apply our model to large industrial firms in Israel and show that it predicts their actual responses to TOU rates quite well.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1679

## 11. Characterization and Generation of a General Class of Resource-Constrained Project Scheduling Problems
### Author(s):
- Rainer Kolisch
- Arno Sprecher
- Andreas Drexl
### Published:
- 1 Oct 1995
### Abstract:
This paper addresses the issue of how to generate problem instances of controlled difficulty. It focuses on precedence- and resource-constrained (project) scheduling problems, but similar ideas may be applied to other network optimization problems. It describes a network construction procedure that takes into account a) constraints on the network topology, b) a resource factor that reflects the density of the coefficient matrix, and c) a resource strength, which measures the availability of resources. The strong impact of the chosen parametric characterization of the problems is shown via an in depth computational study. Instances for the single- and multi-mode resource-constrained project scheduling problem are benchmarked by using the state of the art (branch and bound) procedures. The results provided, demonstrate that the classical benchmark instances used by several researchers over decades belong to the subset of the very easy ones. In addition, it is shown that hard instances, being far more smaller in size than presumed in the literature, may not be solved to optimality even within a large amount of computation time.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1693

## 12. Variability Functions for Parametric-Decomposition Approximations of Queueing Networks
### Author(s):
- Ward Whitt
### Published:
- 1 Oct 1995
### Abstract:
We propose an enhancement to the parametric-decomposition method for calculating approximate steady-state performance measures of open queueing networks with non-Poisson arrival processes and nonexponential service-time distributions. Instead of using a variability parameterca2 for each arrival process, we suggest using a variability functionca2, 0 <  < 1, for each arrival process i.e, the variability parameter should be regarded as a function of the traffic intensity  of a queue to which the arrival process might go. Variability functions provide a convenient representation of different levels of variability in different time scales for arrival processes that are not nearly renewal processes. Variability functions enable the approximations to account for long-range effects in queueing networks that cannot be addressed by variability parameters. For example, the variability functions provide a way to address the heavy-traffic bottleneck phenomenon, in which exceptional variability (either high or low) in the input has little impact in a series of queues with low-to-moderate traffic intensities, and then has a big impact when it reaches a later queue with a relatively high traffic intensity. The variability functions also enable the approximations to characterize irregular periodic deterministic external arrival processes in a reasonable way; i.e., if there are no batches, then ca2 should be 0 for  near 0 or 1, but ca2 can assume arbitrarily large values for appropriate intermediate . We present a full network algorithm with variability functions, showing that the idea is implementable. We also show how simulations of single queues can be effectively exploited to determine variability functions for difficult external arrival processes.
### Link:
- https://doi.org/10.1287/mnsc.41.10.1704


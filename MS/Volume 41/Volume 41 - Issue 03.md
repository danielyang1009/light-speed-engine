# Volume 41, Issue 03
- March 1995
- Pages 377-559
- David Simchi-Levi

## 1. An Experimental Study of Buyer-Seller Negotiation with One-Sided Incomplete Information and Time Discounting
### Author(s):
- Amnon Rapoport
- Ido Erev
- Rami Zwick
### Published:
- 1 Mar 1995
### Abstract:
We study a multiperiod bargaining mechanism in which a seller negotiates with a buyer over the price of an indivisible good. It is common knowledge that the good has zero value to the seller. Its value to the buyer is privately known, distributed independently of the sellers value according to a distribution that is common knowledge. Bargaining proceeds as follows. The seller sets a price and offers the buyer an opportunity to purchase the good. The buyer either waits for at least one more period or agrees to purchase the good at the given price. If the buyer refuses the offer, then the process is repeated with seller making a new offer on the next period. Our findings reveal several behavioral regularities, which do not support the sequential equilibrium for this bargaining mechanism. In line with recent developments in behavioral decision theory and game theory, which assume bounded rationality, we find that subjects follow simple rules of thumb in choosing strategies, reflected in the behavioral consistencies observed in the study.
### Link:
- https://doi.org/10.1287/mnsc.41.3.377

## 2. Effects of Warranty Execution on Warranty Reserve Costs
### Author(s):
- Jayprakash G. Patankar
- Amitava Mitra
### Published:
- 1 Mar 1995
### Abstract:
Previous research has usually assumed full execution of warranty if a product fails within the warranty time. This paper investigates the effect of warranty execution on the expected warranty reserves of a linear pro rata rebate plan. The failure distribution investigated includes the Weibull distribution with different parameter values. A weight function is used that depicts the effect of warranty execution. Several forms of this warranty execution modeling consumer behavior, which is influenced by factors such as the rebate plan, warranty time, product class, and warranty attrition, are studied. Expected present value of warranty reserves, by using a factor that includes the firms discount rate and the general inflation rate, are found for the different warranty execution functions. The impact of the warranty execution function on the warranty reserves per unit is explored. The paper also investigates the effects of the model parameters, such as the failure distribution parameters and differences in consumer behavior in claiming warranties on warranty costs per unit.
### Link:
- https://doi.org/10.1287/mnsc.41.3.395

## 3. Electronic Data Interchange: Competitive Externalities and Strategic Implementation Policies
### Author(s):
- Eric T. G. Wang
- Abraham Seidmann
### Published:
- 1 Mar 1995
### Abstract:
Electronic Data Interchange (EDI) is an emerging type of standardized inter-organizational information system. We analyze the impact of EDI on the upstream suppliers competitive position in a simple two-level hierarchical market structure where the buyer faces a linear demand curve and the competing heterogeneous suppliers have an upward-sloping marginal cost function. We show that a suppliers adoption of EDI can generate positive externalities for the buyer and negative (or competitive) externalities for other suppliers. As a result, the buyer provides a price premium to those suppliers who adopt EDI and increases their sales volume and market share. Moreover, when the benefits that the buyer can derive from implementing EDI are substantial, and the suppliers EDI adoption costs are high, it may be in the buyers best interest to subsidize the suppliers so as to encourage them to adopt EDI, instead of mandating them to do so. Regardless of whether the buyer employs a mandatory or a subsidizing policy, the buyer and the end consumers may be the only ones who gain from this new technology. Consequently, a partial adoption by the supplier base may be optimal for the buyer when the suppliers adoption costs are sufficiently high. We also show that, while EDI reduces the transaction costs of the buyer, the upstream market tends to become more concentrated as a result of increased cost differentials. These results provide one economic explanation of the fact that many companies have actually reduced their supplier base after implementing EDI, despite a significant reduction in their market transaction costs.
### Link:
- https://doi.org/10.1287/mnsc.41.3.401

## 4. Modeling as Constrained Problem Solving: An Empirical Study of the Data Modeling Process
### Author(s):
- Ananth Srinivasan
- Dov Teeni
### Published:
- 1 Mar 1995
### Abstract:
Modeling is a powerful tool for managing complexity in problem solving. Problem solvers usually build a simplified model of the real world and then use it to generate a solution to their problem. To date, however, little is known about how people actually behave when building a model. This study concentrates on data modeling, which involves the representation of different types of data and their interrelationships. It reports on two laboratory studies, in which subjects engage in data modeling to solve a complex problem. Using a think-aloud process-tracing methodology, we examine the data modeling behavior as a set of activities that are managed by several heuristics. We found that some heuristics were effective in reducing the complexity of the problem. An important aspect we observed was how subjects moved across levels of abstraction in the problem representation. Overall, these observations help to explain how people deal with complexity in data modeling. They also suggest that it may be advantageous to design systems that support work at various levels of abstraction and support transitions among those levels.
### Link:
- https://doi.org/10.1287/mnsc.41.3.419

## 5. Two-Person Ratio Efficiency Games
### Author(s):
- J. J. Rousseau
- J. H. Semple
### Published:
- 1 Mar 1995
### Abstract:
This paper demonstrates that a class of two-person games with ratio payoff functions can be solved using equivalent primal-dual linear programming formulations. The games solution contains specialized information which may be used to conduct the efficiency evaluation currently done by the CCR ratio model of Data Envelopment Analysis (DEA). Consequently a rigorous connection between DEAs CCR model and the theory of games is established. Interpretations of these new solutions are discussed in the context of current ongoing applications.
### Link:
- https://doi.org/10.1287/mnsc.41.3.435

## 6. Chance Constrained Efficiency Evaluation
### Author(s):
- O. B. Olesen
- N. C. Petersen
### Published:
- 1 Mar 1995
### Abstract:
A model for efficiency evaluation based upon the theory of chance constrained programming is developed. The model uses a piecewise linear envelopment of confidence regions for observed stochastic multiple-input multiple-output combinations in the Data Envelopment Analysis (DEA) tradition. The model allows for an exogenous decomposition of the total variation in data for each Decision Making Unit (DMU). By varying certain probability levels the model can provide estimates of the sensitivity of efficiency scores regarding an unknown amount of noise in date. An application of the method in an evaluation of the research activities in economic departments at Danish Universities is presented.
### Link:
- https://doi.org/10.1287/mnsc.41.3.442

## 7. From Project to Process Management: An Empirically-Based Framework for Analyzing Product Development Time
### Author(s):
- Paul S. Adler
- Avi Mandelbaum
- Vin Nguyen
- Elizabeth Schwerer
### Published:
- 1 Mar 1995
### Abstract:
While product development efforts are often viewed a unique configurations of idiosyncratic tasks, in reality different projects within an organization often exhibit substantial similarity in the flow of their constituent activities. Moreover, while most of the planning tools available to managers assume that projects are independent clusters of activities, in reality many organizations must manage concurrent projects that place competing demands on shared human and technical resources. This study develops an empirically-based framework for analyzing development time in such contexts. We model the product development organization as a stochastic processing network in which engineering resources are workstations and projects are jobs that flow between the workstations. At any given time, a job is either receiving service or queueing for access to a resource. Our models spreadsheets quantify this division of time, and our simulation experiments investigate the determinants of development cycle time. This class of models provides a useful managerial framework for studying product development because it enables formal performance analysis, and it points to data that should be collected by organizations seeking to improve development cycle times. Such models also provide a conceptual framework for characterizing commonalities and differences between engineering and manufacturing operations.
### Link:
- https://doi.org/10.1287/mnsc.41.3.458

## 8. Mixed-Model Sequencing to Minimize Utility Work and the Risk of Conveyor Stoppage
### Author(s):
- Li-Hui Tsai
### Published:
- 1 Mar 1995
### Abstract:
This paper investigates the problem of sequencing N products on an assembly line with two objectives: minimizing (1) the risk of conveyor stoppage and (2) the total utility work. For a single station with arbitrary processing times, this problem is proved NP-hard in the strong sense for each of the two objectives. For a single station with two product types, each of which has a constant processing time, a sequence minimizing both objectives can be found in O(log N) computation time.
### Link:
- https://doi.org/10.1287/mnsc.41.3.485

## 9. An Integer Programming Model for Locating Vehicle Emissions Testing Stations
### Author(s):
- Arthur J. Swersey
- Lakshman S. Thakur
### Published:
- 1 Mar 1995
### Abstract:
Connecticut and other states not in compliance with federal air quality standards are required to implement a motor vehicle inspection program to test vehicles for pollutantshydrocarbons and carbon monoxide. The problem is to determine the number, size, and locations of stations given constraints on the maximum travel distance from each town to its nearest station and the average waiting time at a station. In this paper we use simulation to find the maximum allowable arrival rates (in vehicles per hour) of stations of different sizes and formulate the station location problem as a set covering model. We generate a range of solutions through sensitivity analysis, varying both the average waiting time and maximum distance constraints. Comparing the current configuration of stations in Connecticut to our integer programming solutions we find that the integer programming approach reduces the objective function by at least $3 million. The current configuration has more stations than the IP solutions but they are not as well distributed.
### Link:
- https://doi.org/10.1287/mnsc.41.3.496

## 10. Measuring the Impact of a Delay Buffer on Quality Costs with an Unreliable Production Process
### Author(s):
- Kamran Moinzadeh
- T. D. Klastorin
### Published:
- 1 Mar 1995
### Abstract:
In this paper, we consider an unreliable production process which produces nondefective items when operating in control, but produces defective items with a probability  when the process has shifted to an out-of-control state. Following a JIT philosophy, we stop the entire line and repair the machine as soon as detect that the process has shifted to an out-of-control state. To test whether a process shift has occurred, we inspect the last m units for every n units produced and stop the machine if a defective unit is found. More important, we place a delay buffer immediately after the unreliable process, which serves to delay the movement of items from the unreliable machine to other processes (or customers) downstream in the production system. When we detect that the machine has shifted to an out-of-control state, we stop the entire line and examine all previously uninspected items in the delay buffer; in this way, the buffer serves to reduce the expected rework and penalty (e.g., warranty) costs downstream when a process shift has occurred. In this paper, we develop a model for this approach and use this model to test the operating characteristics of our system. Computational results illustrate our hypothesis that a delay buffer may significantly reduce expected total costs of a quality control process.
### Link:
- https://doi.org/10.1287/mnsc.41.3.513

## 11. Asymptotics of Likelihood Ratio Derivative Estimators in Simulations of Highly Reliable Markovian Systems
### Author(s):
- Marvin K. Nakayama
### Published:
- 1 Mar 1995
### Abstract:
We discuss the estimation of derivatives of a performance measure using the likelihood ratio method in simulations of highly reliable Markovian systems. We compare the difficulties of estimating the performance measure and of estimating its partial derivatives with respect to component failure rates as the component failure rates tend to 0 and the component repair rates remain fixed. We first consider the case when the quantities are estimated using naive simulation; i.e., when no variance reduction technique is used. In particular, we prove that in the limit, some of the partial derivatives can be estimated as accurately as the performance measure itself. This result is of particular interest in light of the somewhat pessimistic empirical results others have obtained when applying the likelihood ratio method to other types of systems. However, the result only holds for certain partial derivatives of the performance measure when using naive simulation. More specifically, we can estimate a certain partial derivative with the same relative accuracy as the performance measure if the partial derivative is associated with a component either having one of the largest failure rates or whose failure can trigger a failure transition on one of the most likely paths to failure. Also, we develop a simple criterion to determine which partial derivatives will satisfy either of these properties. In particular, we can identify these derivatives using a sensitivity measure which can be calculated for each type of component.
### Link:
- https://doi.org/10.1287/mnsc.41.3.524

## 12. The Unpredictability of Standard Back Propagation Neural Networks in Classification Applications
### Author(s):
- Shouhong Wang
### Published:
- 1 Mar 1995
### Abstract:
This note offers an extension of Tam and Kiang (Tam, K. Y., M. Y. Kiang. 1992. Management applications of neural networks: The case of bank failure predictions. Management Sci.38(7) 926947.). First the weakness of the standard back propagation neural network learning algorithm is discussed, and then a warning is issued regarding applications of artificial neural networks in the management science field. Also suggested is a possible way of improving the performance of neural networks in managerial applications.
### Link:
- https://doi.org/10.1287/mnsc.41.3.555


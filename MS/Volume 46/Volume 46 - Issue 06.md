# Volume 46, Issue 6
- June 2000
- Pages 745-873
- David Simchi-Levi

## 1. An Empirical Analysis of Productivity and Quality in Software Products
### Author(s):
- M. S. Krishnan
- C. H. Kriebel
- Sunder Kekre
- Tridas Mukhopadhyay
### Published:
- 1 Jun 2000
### Abstract:
We examine the relationship between life-cycle productivity and conformance quality in software products. The effects of product size, personnel capability, software process, usage of tools, and higher front-end investments on productivity and conformance quality were analyzed to derive managerial implications based on primary data collected on commercial software projects from a leading vendor. Our key findings are as follows. First, our results provide evidence for significant increases in life-cycle productivity from improved conformance quality in software products shipped to the customers. Given that the expenditure on computer software has been growing over the last few decades, empirical evidence for cost savings through quality improvement is a significant contribution to the literature. Second, our study identifies several quality drivers in software products. Our findings indicate that higher personnel capability, deployment of resources in initial stages of product development (especially design) and improvements in software development process factors are associated with higher quality products.
### Link:
- https://doi.org/10.1287/mnsc.46.6.745.11941

## 2. Revenue Management Without Forecasting or Optimization: An Adaptive Algorithm for Determining Airline Seat Protection Levels
### Author(s):
- Garrett van Ryzin
- Jeff McGill
### Published:
- 1 Jun 2000
### Abstract:
We investigate a simple adaptive approach to optimizing seat protection levels in airline revenue management systems. The approach uses only historical observations of the relative frequencies of certain seat-filling events to guide direct adjustments of the seat protection levels in accordance with the optimality conditions of Brumelle and McGill (1993). Stochastic approximation theory is used to prove the convergence of this adaptive algorithm to the optimal protection levels. In a simulation study, we compare the revenue performance of this adaptive approach to a more traditional method that combines a censored forecasting method with a common seat allocation heuristic (EMSR-b).
### Link:
- https://doi.org/10.1287/mnsc.46.6.760.11936

## 3. Information, Contracting, and Quality Costs
### Author(s):
- Stanley Baiman
- Paul E. Fischer
- Madhav V. Rajan
### Published:
- 1 Jun 2000
### Abstract:
This article analyzes the relation between product quality, the cost of quality, and the information that can be contracted upon. We consider a setting where a risk neutral supplier sells an intermediate product to a risk neutral buyer. The supplier incurs prevention costs to reduce the probability of selling a defective product, and the buyer incurs appraisal costs to identify defects. Both decisions are subject to moral hazard. We show that the first-best outcome can be obtained if either: (i) the supplier's prevention decision is contractible; or (ii) the buyer's appraisal decision and either internal failure (i.e., the product's failing the buyer's appraisal test) or external failure (i.e., the product's failing after being sold by the buyer) are contractible events; or (iii) both internal and external failure are contractible events. We then focus on the second-best setting where actions and failures are not contractible and study the effect of making the buyer's appraisal result contractible. Relative to first-best, if a buyer's return decision is contractible (but not his appraisal result), the supplier incurs lower prevention costs, the buyer incurs higher appraisal costs, expected internal failure costs are higher, and the total cost of quality is higher. The expected costs of external failure, however, may actually be lower relative to first-best. We then show that installing an information system that makes the appraisal result contractible reduces the inefficiency associated with the seller's prevention activity, increases the inefficiency associated with the buyer's quality appraisal activity, and unambiguously improves product quality.
### Link:
- https://doi.org/10.1287/mnsc.46.6.776.11939

## 4. Accounting Information, Aggregation, and Discriminant Analysis
### Author(s):
- Anil Arya
- John Fellingham
- Doug Schroeder
### Published:
- 1 Jun 2000
### Abstract:
Aggregation is a pervasive theme in accounting. The preparation of financial statements involves extensive aggregation—information regarding several transactions is summarized using a few account balances. In this article, we study linear, double-entry aggregation rules. The level of aggregation (transactions versus account balance information) affects a decision maker's ability to discriminate between two entities. We show that the orientation of the discriminant function relative to the row space and the nullspace (two fundamental subspaces) of the double-entry matrix determines the information loss due to aggregation. In addition, we observe that an interdependency in account balances is introduced by the double-entry process. The cause and effect property (debit and credit) translates into a negative covariance being introduced among account balances; this, in turn, affects the decision maker's optimal use of information. Finally, in discussing benefits to aggregation, we present an example in which adopting a double-entry aggregation rule serves as a commitment device for the owner.
### Link:
- https://doi.org/10.1287/mnsc.46.6.790.11935

## 5. Measuring the Robustness of Empirical Efficiency Valuations
### Author(s):
- Ludwig Kuntz
- Stefan Scholtes
### Published:
- 1 Jun 2000
### Abstract:
We study the robustness of empirical efficiency valuations of production processes in an extended Farrell model. Based on input and output data, an empirical efficiency status—efficient or inefficient—is assigned to each of the processes. This status may change if the data of the observed processes change. As illustrated by a capacity planning problem for hospitals in Germany, the need arises to gauge the robustness of empirical efficiency valuations. The example suggests to gauge the robustness of the efficiency valuation for a process with respect to perturbations of prespecified elements of the data. A natural measure of robustness is the minimal perturbation, in terms of a suitable distance function, of the chosen data elements that is necessary to change the efficiency status of the process under investigation. Farrell's (1957) efficiency score is an example of such a robustness measure. We give further examples of relevant data perturbations for which the robustness measure can be computed efficiently. We then focus on weighted maximum norm distance functions, such as the maximal absolute or percentage deviation, but allow for independent perturbations of the elements of an arbitrary a priori fixed subset of the data. In this setting, the robustness measure is naturally related to a certain threshold value for a linear monotone one-parameter family of perturbations and can be calculated by means of a linear programming–based bisection method. Closed form solutions in terms of Farrell's efficiency score are obtained for specific perturbations. Following the theoretical developments, we revisit the hospital capacity planning problem to illustrate the managerial relevance of our techniques.
### Link:
- https://doi.org/10.1287/mnsc.46.6.807.11937

## 6. Imitation of Complex Strategies
### Author(s):
- Jan W. Rivkin
### Published:
- 1 Jun 2000
### Abstract:
Researchers examining loosely coupled systems, knowledge management, and complementary practices in organizations have proposed, informally, that the complexity of a successful business strategy can deter imitation of the strategy. This paper explores this proposition rigorously. A simple model is developed that parametrizes the two aspects of strategic complexity: the number of elements in a strategy and the interactions among those elements. The model excludes conventional resource-based and game-theoretic barriers to imitation altogether. The model is used to show that complexity makes the search for an optimal strategy intractable in the technical sense of the word provided by the theory of NP-completeness. Consequently, would-be copycats must rely on search heuristics or on learning, not on algorithmic “solutions,” to match the performance of superior firms. However, complexity also undermines heuristics and learning. In the face of complexity, firms that follow simple hill-climbing heuristics are quickly snared on low “local peaks,” and firms that try to learn and mimic a high performer's entire strategy suffer large penalties from small errors. The model helps to explain why some winning strategies remain unmatched even though they are open to public scrutiny; why certain bundles of organizational practices diffuse slowly even though they lead to superior performance; and why some strategies yield superior returns even after many of their critical ingredients are adopted by competitors. The analysis also suggests roles for management science and managerial choice in a world of complex strategies.
### Link:
- https://doi.org/10.1287/mnsc.46.6.824.11940

## 7. Optimal Parallel Inspection for Finding the First Nonconforming Unit in a Batch—An Information Theoretic Approach
### Author(s):
- Yale T. Herer
- Tzvi Raz
### Published:
- 1 Jun 2000
### Abstract:
We consider the case of a batch of discrete units produced by a process subject to failures under a known probability distribution function, and apply information theory to the problem of finding the first nonconforming unit in the batch at minimum cost. Two distinct but related aspects of this problem were treated: determining which units should be inspected, and determining how many units should be sent for inspection at the same time. The solution is based on the principles of inspecting the product units that maximize the reduction in the uncertainty regarding the location of the first nonconforming unit, and of minimizing the cost per unit of uncertainty reduced. These principles are formalized by means of a series of theorems leading to an easy-to-implement algorithm for managing parallel inspection. This approach is successfully compared with the optimal solution obtained with dynamic programming and with other heuristics.
### Link:
- https://doi.org/10.1287/mnsc.46.6.845.11933

## 8. Convex Input and Output Projections of Nonconvex Production Possibility Sets
### Author(s):
- Peter Bogetoft
- Joseph M. Tama
- Jørgen Tind
### Published:
- 1 Jun 2000
### Abstract:
In this paper we characterize the smallest production possibility set that contains a specified set of (input, output) combinations. In accordance with neoclassical production economics, this possibility set has convex projections into the input and output spaces (convex isoquants), and it satisfies the assumption of free disposability. We obtain it by means of a possibly infinite recursion which builds the possibility set as an ever larger union of convex sets. We remark on the nature of the approximations obtained by truncating the recursion, and we obtain a necessary and sufficient condition, checkable in one iteration for the recursion to stop in the next. For the case in which the recursion stops, we provide a succinct characterization of the dominance relations among the constituent sets produced by the procedure. Finally, we present examples of both finite and infinite cases. The example for the finite case illustrates the construction of the possibility set along with its associated production and consumption sets.
### Link:
- https://doi.org/10.1287/mnsc.46.6.858.11938

## 9. A Remark on Third Degree Stochastic Dominance
### Author(s):
- Man-Chung Ng
### Published:
- 1 Jun 2000
### Abstract:
This note presents two counterexamples to illustrate that neither implication of Theorem 4 in Levy (1992) is correct.
### Link:
- https://doi.org/10.1287/mnsc.46.6.870.11934


# Volume 47, Issue 3
- March 2001
- Pages 337-491
- David Simchi-Levi

## 1. Entrepreneurs, Contracts, and the Failure of Young Firms
### Author(s):
- Pierre Azoulay
- Scott Shane
### Published:
- 1 Mar 2001
### Abstract:
Although economic theory has emphasized that moral hazard and hold-up problems influence the design of contracts, very little is known about the process by which explicit contracts are established and the effect of contractual arrangements on firm performance. This paper attempts to demonstrate that firms are selected for survival on the basis of contracting efficiency. Based on a statistical analysis of 170 new franchise contracts and interviews with the founders of 16 of these new franchise systems, we show that new franchise chains that adopt exclusive territories are more likely to survive over time than chains that do not. Moreover, successful and failed entrepreneurs possess different information about how to design contracts. These entrepreneurs undertake “contractual experiments” based on the information they possess. Those whose experiments prove to be more consistent with economic theory are rewarded for their superior information with survival.
### Link:
- https://doi.org/10.1287/mnsc.47.3.337.9771

## 2. The Long-Run Stock Price Performance of Firms with Effective TQM Programs
### Author(s):
- Kevin B. Hendricks
- Vinod R. Singhal
### Published:
- 1 Mar 2001
### Abstract:
This paper documents the long-run stock price performance of firms with effective Total Quality Management (TQM) programs. The winning of quality awards is used as a proxy for effective TQM implementation. We compare stock price performance of award winners against various matched control groups for a five-year implementation period and a five-year postimplementation period. During the implementation period there is no difference in the stock price performance, but during the postimplementation period award winners significantly outperform firms in the various control groups. Depending on the control group used, the mean outperformance ranges from 38% to 46%. Our results clearly indicate that effective implementation of TQM principles and philosophies leads to significant wealth creation. Furthermore, our results should alleviate many of the concerns regarding the value of quality award systems. Overall, these systems are valuable in terms of recognizing TQM firms and promoting awareness of TQM.
### Link:
- https://doi.org/10.1287/mnsc.47.3.359.9773

## 3. March Madness and the Office Pool
### Author(s):
- Edward H. Kaplan
- Stanley J. Garstka
### Published:
- 1 Mar 2001
### Abstract:
March brings March Madness, the annual conclusion to the U.S. men's college basketball season with two single elimination basketball tournaments showcasing the best college teams in the country. Almost as mad is the plethora of office pools across the country where the object is to pick a priori as many game winners as possible in the tournament. More generally, the object in an office pool is to maximize total pool points, where different points are awarded for different correct winning predictions. We consider the structure of single elimination tournaments, and show how to efficiently calculate the mean and the variance of the number of correctly predicted wins (or more generally the total points earned in an office pool) for a given slate of predicted winners. We apply these results to both random and Markov tournaments. We then show how to determine optimal office pool predictions that maximize the expected number of points earned in the pool. Considering various Markov probability models for predicting game winners based on regular season performance, professional sports rankings, and Las Vegas betting odds, we compare our predictions with what actually happened in past NCAA and NIT tournaments. These models perform similarly, achieving overall prediction accuracies of about 58%, but do not surpass the simple strategy of picking the seeds when the goal is to pick as many game winners as possible. For a more sophisticated point structure, however, our models do outperform the strategy of picking the seeds.
### Link:
- https://doi.org/10.1287/mnsc.47.3.369.9769

## 4. Pricing Discrete Barrier and Hindsight Options with the Tridiagonal Probability Algorithm
### Author(s):
- Wai Man Tse
- Leong Kwan Li
- Kai Wang Ng
### Published:
- 1 Mar 2001
### Abstract:
This paper develops an algorithm to calculate the Brownian multivariate normal probability subject to any preset error tolerance criteria. The algorithm is founded upon the computational simplicity of the tridiagonal structure of the inverse of the Brownian correlation matrix. Compared with existing pricing technologies without the “barrier too close” problem, our calculation method can produce a more accurate and efficient analytic evaluation of barrier options monitored at discrete instants with well- or ill-behaved barrier levels, or discrete hindsight options, for a reasonably large number of monitorings.
### Link:
- https://doi.org/10.1287/mnsc.47.3.383.9775

## 5. One Size Does Not Fit All Projects: Exploring Classical Contingency Domains
### Author(s):
- Aaron J. Shenhar
### Published:
- 1 Mar 2001
### Abstract:
Not many authors have attempted to classify projects according to any specific scheme, and those who have tried rarely offered extensive empirical evidence. From a theoretical perspective, a traditional distinction between radical and incremental innovation has often been used in the literature of innovation, and has created the basis for many classical contingency studies. Similar concepts, however, did not become standard in the literature of projects, and it seems that theory development in project management is still in its early years. As a result, most project management literature still assumes that all projects are fundamentally similar and that “one size fits all.” The purpose of this exploratory research is to show how different types of projects are managed in different ways, and to explore the domain of traditional contingency theory in the more modern world of projects. This two-step research is using a combination of qualitative and quantitative methods and two data sets to suggest a conceptual, two-dimensional construct model for the classification of technical projects and for the investigation of project contingencies. Within this framework, projects are classified into four levels of technological uncertainty, and into three levels of system complexity, according to a hierarchy of systems and subsystems. The study provides two types of implications. For project leadership it shows why and how management should adapt a more project-specific style. For theory development, it offers a collection of insights that seem relevant to the world of projects as temporary organizations, but are, at times, different from classical structural contingency theory paradigms in enduring organizations. While still exploratory in nature, this study attempts to suggest new inroads to the future study of modern project domains.
### Link:
- https://doi.org/10.1287/mnsc.47.3.394.9772

## 6. A Periodic Review Inventory System with Emergency Replenishments
### Author(s):
- George Tagaras
- Dimitrios Vlachos
### Published:
- 1 Mar 2001
### Abstract:
This paper proposes and analyzes a periodic review inventory system with two replenishment modes. Regular orders are placed periodically following a base stock policy on inventory position, and arrive at the stocking location after a deterministic lead time. The location also has the option of placing emergency orders, characterized by a shorter lead time but higher acquisition cost, in case of imminent stockouts. Thus, at some appropriate time in the replenishment cycle, the necessity and size of an emergency order is determined according to a base stock policy on net stock. The timing of the emergency order is such that this order arrives and can be used to satisfy the demand in the time period just before the arrival of a regular order, when the likelihood of a stockout is highest. An approximate cost model is developed which can easily be optimized with respect to the order-up-to parameters. This model is used as the basis for a heuristic algorithm, which leads to solutions that are very close to the exact optimal solutions determined through simulation. It is shown that the proposed system offers substantial cost savings relative to a system without the emergency replenishment option.
### Link:
- https://doi.org/10.1287/mnsc.47.3.415.9770

## 7. The Dynamic Value of Hierarchy
### Author(s):
- Anne Marie Knott
### Published:
- 1 Mar 2001
### Abstract:
This study develops a dual-routines view of the dynamic value of hierarchy, and tests it against the implicit null hypothesis that hierarchy merely provides static advantages over markets. The view holds that hierarchical managers perform two roles that create value for firms in perpetuity—an administrative role of enforcing operational routine, and an entrepreneurial role of executing a metaroutine that continually revises operational routine to keep pace with changes in the environment. The test consists of a natural experiment comparing the behavior and performance of establishments that leave a franchise, “lose their hierarchical managers,” with those that remain.
### Link:
- https://doi.org/10.1287/mnsc.47.3.430.9776

## 8. Comparisons with a Standard in Simulation Experiments
### Author(s):
- Barry L. Nelson
- David Goldsman
### Published:
- 1 Mar 2001
### Abstract:
We consider the problem of comparing a finite number of stochastic systems with respect to a single system (designated as the “standard”) via simulation experiments. The comparison is based on expected performance, and the goal is to determine if any system has larger expected performance than the standard, and if so to identify the best of the alternatives. In this paper we provide two-stage experiment design and analysis procedures to solve the problem for a variety of scenarios, including those in which we encounter unequal variances across systems, as well as those in which we use the variance reduction technique of common random numbers and it is appropriate to do so. The emphasis is added because in some cases common random numbers can be counterproductive when performing comparisons with a standard. We also provide methods for estimating the critical constants required by our procedures, present a portion of an extensive empirical study, and demonstrate one of the procedures via a numerical example.
### Link:
- https://doi.org/10.1287/mnsc.47.3.449.9778

## 9. Managing Inventory with Multiple Products, Lags in Delivery, Resource Constraints, and Lost Sales: A Mathematical Programming Approach
### Author(s):
- Brian Downs
- Richard Metters
- John Semple
### Published:
- 1 Mar 2001
### Abstract:
This paper develops an order-up-to S inventory model that is designed to handle multiple items, resource constraints, lags in delivery, and lost sales without sacrificing computational simplicity. Mild conditions are shown to ensure that the expected average holding cost and the expected average shortage cost are separable convex functions of the order-up-to levels. We develop nonparametric estimates of these costs and use them in conjunction with linear programming to produce what is termed the “LP policy.” The LP policy has two major advantages over traditional methods: first, it can be computed in complex environments such as the one described above; and second, it does not require an explicit functional form of demand, something that is difficult to specify accurately in practice. In two numerical experiments designed so that optimal policies could be computed, the LP policy fared well, differing from the optimal profit by an average of 2.20% and 1.84%, respectively. These results compare quite favorably with the errors incurred in traditional methods when a correctly specified distribution uses estimated parameters. Our findings support the effectiveness of this mathematical programming technique for approximating complex, real-world inventory control problems.
### Link:
- https://doi.org/10.1287/mnsc.47.3.464.9774

## 10. Sequencing JIT Mixed-Model Assembly Lines Under Station-Load and Part-Usage Constraints
### Author(s):
- Andreas Drexl
- Alf Kimms
### Published:
- 1 Mar 2001
### Abstract:
This paper deals with two most important problems, from both practical and theoretical standpoints, arising in sequencing mixed-model assembly lines. Such lines have become core components of modern repetitive manufacturing, and just-in-time (JIT) manufacturing in particular. One problem is to keep the usage rate of all parts fed into the final assembly as constant as possible (the “level-scheduling problem”), while the other is to keep the line's workstation loads as constant as possible (the “car-sequencing problem”). In this paper the combined problem is formulated as a single-integer programming model. The LP-relaxation of this model is solved by column-generation techniques. The results of an experimental evaluation show that the lower bounds are tight.
### Link:
- https://doi.org/10.1287/mnsc.47.3.480.9777

